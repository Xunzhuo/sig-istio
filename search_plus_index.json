{"./":{"url":"./","title":"欢迎","keywords":"","body":"云原生社区 Istio SIG 欢迎来到云原生社区 Istio SIG（特别兴趣小组）！ 进入 Istio SIG 页面：https://cloudnative.to/sig-istio/ 加入 Istio SIG 微信群 请填写申请表加入微信交流群和参与社区活动。 联系 请联系负责人：宋净超（Jimmy Song）。 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"begin/how-istio-got-its-name.html":{"url":"begin/how-istio-got-its-name.html","title":"Istio 名称的来历","summary":"本文将向你介绍 Istio 何以得名？","keywords":"","body":"Istio 名称的来历 Istio（ISS-tee-oh）是由 Tetrate 创始人 Varun Talwar 和谷歌首席工程师 Louis Ryan 在 2017 年命名的，当时他们都在谷歌工作。 Istio 在希腊语中是 “sail” 的意思，它（ιστίο）延用了 Kubernetes（在希腊语中是飞行员或舵手的意思）建立的希腊航海主题。Istio 和它的表亲 Istos（ιστός）（意思是桅杆、网）都来自古希腊词根 Istimi（ἵστημι），意思是 “to make stand”。 “为了取名，我们翻阅了三个小时希腊字典”Varun 说，“最终我们两人想到一起去了。帆船的理念不仅在于谁在控制船，而是没有船你哪儿也去不了。“ Varun 回忆说 10 个候选名称最后被缩减到 3 个，但命名 Istio Github 仓库的实际需求要求他们快速做出最终决定。 Istio 现在的网址是 istio.io，“io” 的重复是值得商榷。就其本身而言，“io” 有很多含义，包括 ionium（锾元素）的缩写、以古代宙斯的情人命名的木星卫星，以及得名于印度洋的互联网顶级域名。我们也注意到 io 是 Ino（奥德赛中的人名）的另一种拼法，Ino 给了 Odysseus（奥德赛中的人名）一个神奇的面纱，让他在五级海洋风暴中顺利航行。Istio 不正是那件法器吗？ 最后说明一下：Istio 并不是首字母缩写，但如果一定要说它代表着什么，也许会是 “I Secure, Then I Observe”（我保护，我观察），或者 \"I’m Sexy To Infrastructure Operators.\"（我对基础设施运营商富有吸引力）。 语言学家 Efthymia Lixourgioti 为 Istio 项目的取名提供了希腊语翻译和词源学的咨询。 参考 How Istio got its name - tetrate.io 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"begin/do-you-really-need-istio.html":{"url":"begin/do-you-really-need-istio.html","title":"你是否真的需要 Istio？","summary":"在开始学习和使用 Istio 之前，你不妨先考虑以下问题。","keywords":"","body":"你是否真的需要 Istio？ 你可能参加过各种云原生、服务网格相关的 meetup，在社区里看到很多人在分享和讨论 Istio，但是对于自己是否真的需要 Istio 感到踌躇，甚至因为它的复杂性而对服务网格的前景感到怀疑。那么，在你继阅读 Istio SIG 后续文章之前，请先仔细阅读本文，审视一下自己公司的现状，看看你是否有必要使用服务网格，处于 Istio 应用的哪个阶段。 本文不是对应用服务网格的指导，而是根据社区里经常遇到的问题而整理。在使用 Istio 之前，请先考虑下以下因素： 你的团队里有多少人？ 你的团队是否有使用 Kubernetes、Istio 的经验？ 你有多少微服务？ 这些微服务使用什么语言？ 你的运维、SRE 团队是否可以支持服务网格管理？ 你有采用开源项目的经验吗？ 你的服务都运行在哪些平台上？ 你的应用已经容器化并使用 Kubernetes 管理了吗？ 你的服务有多少是部署在虚拟机、有多少是部署到 Kubernetes 集群上，比例如何？ 你的团队有制定转移到云原生架构的计划吗？ 你想使用 Istio 的什么功能？ Istio 的稳定性是否能够满足你的需求？ 你是否可以忍受 Istio 带来的性能损耗？ 请先思考一下上述问题，关于是否应该使用 Istio，及应用服务网格化的路径，欢迎到云原生社区 Istio SIG 中探讨。 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"begin/before-you-begin.html":{"url":"begin/before-you-begin.html","title":"使用 Istio 前的考虑要素","summary":"本文是笔者深度参与百亿量级流量生产环境研发和落地 Istio 两年来的经验总结和一些思考，以期读者在自己生产环境引入 Isito 前，能有所参考和启发，做好更充足的准备，能更轻松的 “入坑” Istio。","keywords":"","body":"使用 Isito 前的考虑要素 2021 年伊始，如果你想要在生产环境中落地 Service Mesh，那 Istio 一定已经在你的考虑范围之内。 Istio 作为目前最流行的 Service Mesh 技术之一，拥有活跃的社区和众多的落地案例。但如果你真的想在你的生产环境大规模落地 Isito，这看似壮观美好的冰山下，却是暗流涌动，潜藏着无数凶险。 使用 Istio 无法做到完全对应用透明 服务通信和治理相关的功能迁移到 Sidecar 进程中后， 应用中的 SDK 通常需要作出一些对应的改变。 比如 SDK 需要关闭一些功能，例如重试。一个典型的场景是，SDK 重试 m 次，Sidecar 重试 n 次，这会导致 m * n 的重试风暴，从而引发风险。 此外，诸如 trace header 的透传，也需要 SDK 进行升级改造。如果你的 SDK 中还有其它特殊逻辑和功能，这些可能都需要小心处理才能和 Isito Sidecar 完美配合。 Istio 对非 Kubernetes 环境的支持有限 在业务迁移至 Istio 的同时，可能并没有同步迁移至 Kubernetes，而还运行在原有 PAAS 系统之上。 这会带来一系列挑战： 原有 PAAS 可能没有容器网络，Istio 的服务发现和流量劫持都可能要根据旧有基础设施进行适配才能正常工作 如果旧有的 PAAS 单个实例不能很好的管理多个容器（类比 Kubernetes 的 Pod 和 Container 概念），大量 Istio Sidecar 的部署和运维将是一个很大的挑战 缺少 Kubernetes webhook 机制，Sidecar 的注入也可能变得不那么透明，而需要耦合在业务的部署逻辑中 只有 HTTP 协议是一等公民 Istio 原生对 HTTP 协议提供了完善的全功能支持，但在真实的业务场景中，私有化协议却非常普遍，而 Istio 却并未提供原生支持。 这导致使用私有协议的一些服务可能只能被迫使用 TCP 协议来进行基本的请求路由，这会导致很多功能的缺失，这其中包括 Istio 非常强大的基于内容的消息路由，如基于 header、 path 等进行权重路由。 扩展 Istio 的成本并不低 虽然 Istio 的总体架构是基于高度可扩展而设计，但由于整个 Istio 系统较为复杂，如果你对 Istio 进行过真实的扩展，就会发现成本不低。 以扩展 Istio 支持某一种私有协议为例，首先你需要在 Istio 的 api 代码库中进行协议扩展，其次你需要修改 Istio 代码库来实现新的协议处理和下发，然后你还需要修改 xds 代码库的协议，最后你还要在 Envoy 中实现相应的 Filter 来完成协议的解析和路由等功能。 在这个过程中，你还可能面临上述数个复杂代码库的编译等工程挑战（如果你的研发环境不能很好的使用 Docker 或者无法访问部分国外网络的情况下）。 即使做完了所有的这些工作，你也可能面临这些工作无法合并回社区的情况，社区对私有协议的扩展支持度不高，这会导致你的代码和社区割裂，为后续的升级更新带来隐患。 Istio 在集群规模较大时的性能问题 Istio 默认的工作模式下，每个 Sidecar 都会收到全集群所有服务的信息。如果你部署过 Istio 官方的 Bookinfo 示例应用，并使用 Envoy 的 config dump 接口进行观察，你会发现，仅仅几个服务，Envoy 所收到的配置信息就有将近 20w 行。 可以想象，在稍大一些的集群规模，Envoy 的内存开销、Istio 的 CPU 开销、XDS 的下发时效性等问题，一定会变得尤为突出。 Istio 这么做一是考虑这样可以开箱即用，用户不用进行过多的配置，另外在一些场景，可能也无法梳理出准确的服务之间的调用关系，因此直接给每个 Sidecar 下发了全量的服务配置，即使这个 Sidecar 只会访问其中很小一部分服务。 当然这个问题也有解法，你可以通过 Sidecar CRD 来显示定义服务调用关系，使 Envoy 只得到他需要的服务信息，从而大幅降低 Envoy 的资源开销，但前提是在你的业务线中能梳理出这些调用关系。 XDS 分发没有分级发布机制 当你对一个服务的策略配置进行变更的时候，XDS 不具备分级发布的能力，所有访问这个服务的 Envoy 都会立即收到变更后的最新配置。这在一些对变更敏感的严苛生产环境，可能是有很高风险甚至不被允许的。 如果你的生产环境严格要求任何变更都必须有分级发布流程，那你可能需要考虑自己实现一套这样的机制。 Istio 组件故障时是否有退路？ 以 Istio 为代表的 Sidecar 架构的特殊性在于，Sidecar 直接承接了业务流量，而不像一些其他的基础设施那样，只是整个系统的旁路组件（比如 Kubernetes）。 因此在 Isito 落地初期，你必须考虑，如果 Sidecar 进程挂掉，服务怎么办？是否有退路？是否能 fallback 到直连模式？ 在 Istio 落地过程中，是否能无损 fallback，通常决定了核心业务能否接入 Service Mesh。 Isito 技术架构的成熟度还没有达到预期 虽然 Istio 1.0 版本已经发布了很久，但是如果你关注社区每个版本的迭代，就会发现，Istio 目前架构依然处于不太稳定的状态，尤其是 1.5 版本前后的几个大版本，先后经历了去除 Mixer 组件、合并为单体架构、仅支持高版本 Kubernetes 等等重大变动，这对于已经在生产环境中使用了 Istio 的用户非常不友好，因为升级会面临各种不兼容性问题。 好在社区也已经意识到这一问题，2021 年社区也成立了专门的小组，重点改善 Istio 的兼容性和用户体验。 Istio 缺乏成熟的产品生态 Istio 作为一套技术方案，却并不是一套产品方案。 如果你在生产环境中使用，你可能还需要解决可视化界面、权限和账号系统对接、结合公司已有技术组件和产品生态等问题，仅仅通过命令行来使用，可能并不能满足你的组织对权限、审计、易用性的要求。 而 Isito 自带的 Kiali 功能还十分简陋，远远没有达到能在生产环境使用的程度，因此你可能需要研发基于 Isito 的上层产品。 Istio 目前解决的问题域还很有限 Istio 目前主要解决的是分布式系统之间服务调用的问题，但还有一些分布式系统的复杂语义和功能并未纳入到 Istio 的 Sidecar 运行时之中，比如消息发布和订阅、状态管理、资源绑定等等。 云原生应用将会朝着多 Sidecar 运行时或将更多分布式能力纳入单 Sidecar 运行时的方向继续发展，以使服务本身变得更为轻量，让应用和基础架构彻底解耦。 如果你的生产环境中，业务系统对接了非常多和复杂的分布式系系统中间件，Istio 目前可能并不能完全解决你的应用的云原生化诉求。 写在最后 看到这里，你是否感到有些沮丧，而对 Isito 失去信心？ 别担心，上面列举的这些问题，实际上并不影响 Isito 依然是目前最为流行和成功的 Service Mesh 技术选型之一。Istio 频繁的变动，一定程度上也说明它拥有一个活跃的社区，我们应当对一个新的事物报以信心，Isito 的社区也在不断听取来自终端用户的声音，朝着大家期待的方向演进。 同时，如果你的生产环境中的服务规模并不是很大，服务已经托管于 Kubernetes 之上，也只使用那些 Istio 原生提供的能力，那么 Istio 依然是一个值得尝试的开箱即用方案。 但如果你的生产环境比较复杂，技术债务较重，专有功能和策略需求较多，亦或者服务规模庞大，那么在开始使用 Istio 之前，你需要仔细权衡上述这些要素，以评估在你的系统之中引入 Istio 可能带来的复杂度和潜在成本。 作者：陈鹏，原文在生产环境使用 Istio 前的若干考虑要素。 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"event/istio-doc-translation.html":{"url":"event/istio-doc-translation.html","title":"Istio 官方文档翻译","keywords":"","body":"Istio 官方文档翻译 在此之前，Istio 官方文档已经进行了两轮中文翻译活动，第一轮是在 2018 年，基于 Istio 0.8，第二轮是在 2020 年，基于 Istio 1.5，截止目前 Istio 已发布了 1.9 版本，Istio 中文文档已经有长达一年的时间疏于维护，现在云原生社区 Istio SIG 决定重启中文文档的维护。 常用链接 Istio 中文文档地址：https://istio.io/zh 登记及任务认领表：Google Spreadsheet Istio 官网 GitHub 仓库：https://github.com/istio/istio.io 加入协作微信群申请：加入 Istio SIG 负责人 下面是云原生社区 Istio Doc WG 的负责人： 宋净超（@rootsongjc） 刘齐均（@kebe7jun） 殷龙飞（@loverto） 邱世达（@SataQiu） 在参与过程中有任何问题可以与他们联系。 如何参与 本次活动由云原生社区 Istio Doc WG 主办，参与活动需要你准备以下内容。 准备 GitHub 你需要一个 GitHub 账号，翻译文档需要通过 GitHub 提交 PR，需要你熟悉 Git 命令和 GitHub 的基本操作； 登记和任务认领 报名参与和认领任务都在 Google Spreadsheet 中（请务必在表格中登记信息）。 加入微信群 微信是 Istio 官方文档翻译工作组的即时通讯工具，并加入 CNC Istio Doc WG 微信群（加入 Istio SIG 即可加入） 翻译流程 在你完成登记和认领任务之后就可以开始翻译了，下面是翻译流程。 构建本地环境 在克隆了 Istio 文档的仓库后，有两种方式可以将 Istio 的网站在本地运行起来。 通过本地运行 hugo 启动 Hugo 提供了一个本地的 web 服务器，可以启动网站。如果您本地没有安装 hugo，可以去这里查看如何安装。 然后，在 Istio.io 仓库的根目录下，运行 hugo server 在本地启动 web 服务器，通过 http://localhost:1313/latest/zh 进行中文网站的预览。如看到类似下面的输出，则表示 web 服务器已经启动成功： | EN | ZH +------------------+-----+-----+ Pages | 545 | 545 Paginator pages | 0 | 0 Non-page files | 164 | 164 Static files | 54 | 54 Processed images | 0 | 0 Aliases | 1 | 0 Sitemaps | 2 | 1 Cleaned | 0 | 0 Total in 47355 ms Watching for changes in /work/{archetypes,assets,content,data,generated,i18n,layouts,static} Watching for config changes in /work/config.toml Environment: \"development\" Serving pages from memory Web Server is available at http://localhost:1313/ (bind address 0.0.0.0) Press Ctrl+C to stop 通过 Docker 启动 另外一种是直接使用 Docker 镜像启动。 在正确安装 Docker 后，运行下面的命令下载镜像： docker pull gcr.io/istio-testing/build-tools:master-2021-04-12T17-40-14 如果您的网络环境无法访问此资源，可以执行下面的命令下载镜像的镜像： docker pull jimmysong/istio-testing-build-tools:master-2021-04-12T17-40-14 docker tag jimmysong/istio-testing-build-tools:master-2021-04-12T17-40-14 gcr.io/build-tools:master-2021-04-12T17-40-14 然后在 istio.io 仓库的根目录下，执行下面的命令启动 web 服务： make serve 启动成功后通过 http://localhost:1313/latest/zh 进行网站的预览。 如果你想通过局域网访问该页面，可以将 Makefile.core.mk 文件中的 ISTIO_SERVE_DOMAIN ?= localhost 修改为 ISTIO_SERVE_DOMAIN ?= 局域网 IP，然后再启动 web 服务： make serve 这可以让局域网中的其它计算机访问该页面（以及物理机访问虚拟机），注意：不要将该文件的改动提交至 PR。 启动成功后通过 http://局域网 IP:1313/latest/zh 进行网站的预览。 提交 PR 如果检查通过，就可以向 Istio 官方网站提交 PR，PR 被合并后就可以通过 Istio 网站预览页面看到被合并后的页面。为方便管理和辨识，请遵守下面的模板定义您的 PR： 标题： zh-translation: 内容： ref: https://github.com/servicemesher/istio-official-translation/issues/ [ ] Configuration Infrastructure [x] Docs [ ] Installation [ ] Networking [ ] Performance and Scalability [ ] Policies and Telemetry [ ] Security [ ] Test and Release [ ] User Experience [ ] Developer Infrastructure 其中，标题中的 是翻译的源文件路径；内容中的 ref 是当前翻译任务的 issue 链接。 校对 校对工作由没有翻译过当前文档的其他翻译人员执行，即翻译人员互为校对人员。为保证质量，我们设置了两轮 Review： 所有翻译人员互为校对人员，分配一个翻译任务同时要确定校对任务； 初审：负责对翻译的内容和原文较为精细的进行对比，保证语句通顺，无明显翻译错误； 终审：负责对翻译的文档做概要性的检查，聚焦在行文的通顺性、一致性、符合中文语言习惯，词汇、术语准确。终审通过后由管理员 approve 当前 PR，就可以进行合并了。 Review 的基本流程 认领 Review 新提交的 PR 每天会在协作群发布，供大家认领； 进入要认领的 PR，回复 /review，并在 Google Spreadsheet 对应的任务中登记 reviewer； Review 重点 打开 PR 提交的中文翻译，并找到对应 issue 中指定的源文件，逐段进行走查； 词汇检查：检查译文中出现的术语、常用词汇是否遵照了术语表的要求进行翻译； 格式检查：对照原文，检查译文中的标题和层次是否对应；代码块是否指定了语言；标点符号是否正确且无英文标点；超链接、图片链接是否可达；是否有错别字； 语句检查：分段落通读一遍，检查是否有不通顺、语病、或者不符合中文习惯的译文（啰嗦、重复、过多的助词等） 提交 comment：根据发现的问题，在 PR 提交文件的对应行添加 comment，格式为原译文=>修改后译文；不确定的地方可加建议或询问，或发到协作群求助。 更新任务表 通过终审后的任务会被负责人 approve，并合并到 Istio 的官方仓库中。需要您在 Google Spreadsheet 的任务列表中更新所认领的任务的状态。整个翻译任务就算正式完成了。您可以继续领取新的任务进行翻译，或参与校对工作。 FAQ 初次使用 hugo 启动找不到静态资源问题 初次使用 hugo server 在本地启动 web 服务，web 页面会出现如下问题，找不到静态资源。 Failed to load resource: the server responded with a status of 404 (Not Found) Refused to apply style from 'http://localhost:1313/css/all.css' because its MIME type ('text/plain') is not a supported stylesheet MIME type, and strict MIME checking is enabled. 解决方法： 在项目根目录下执行 sh scripts/build_site.sh 命令，即可生成所需静态文件。但是这种方式需要安装比较多 node 的命令行工具，例如：sass、tsc、babel、svgstore，安装起来比较繁琐。 这里建议首次可以采用 docker 方式启动，参考 docker 启动教程，在 Istio 网站仓库的根目录运行 make serve 启动，如果您的网络环境无法访问此资源，请使用 make serve IMG=jimmysong/istio-testing-build-tools:master-2021-03-01T22-30-49 命令，启动时 docker 镜像会在项目目录中生成 generated、tmp 和 resources 静态资源目录。 在初次生成静态资源目录后，就可以正常使用 hugo server 来启动项目了。 定义的锚点报拼写错误 给标题添加的锚点完全和官方英文的一致，报类似如下错误： 错误截图 主要的原因是在对于这些专有名词在.spelling 文件中只定义了大写而没有定义小写导致。此时，请参考上文锚点规范书写锚点。 CI deploy/netlify 报错 本地 make serve 没问题，但官方的 deploy/netlify 报如下错误： 10:07:37 AM: added 35 packages from 9 contributors and audited 43 packages in 1.553s 10:07:37 AM: found 0 vulnerabilities 10:07:41 AM: TypeError: Cannot set property inList of [object Object] which has only a getter 10:07:41 AM: at PluginPass.exit (/opt/build/repo/node_modules/babel-plugin-minify-simplify/lib/index.js:549:40) 10:07:41 AM: at newFn (/opt/buildhome/.nvm/versions/node/v12.8.0/lib/node_modules/@babel/core/node_modules/@babel/traverse/lib/visitors.js:179:21) 10:07:41 AM: at NodePath._call (/opt/buildhome/.nvm/versions/node/v12.8.0/lib/node_modules/@babel/core/node_modules/@babel/traverse/lib/path/context.js:55:20) 10:07:41 AM: at NodePath.call (/opt/buildhome/.nvm/versions/node/v12.8.0/lib/node_modules/@babel/core/node_modules/@babel/traverse/lib/path/context.js:42:17) 10:07:41 AM: at NodePath.visit (/opt/buildhome/.nvm/versions/node/v12.8.0/lib/node_modules/@babel/core/node_modules/@babel/traverse/lib/path/context.js:99:8) 10:07:41 AM: at TraversalContext.visitQueue (/opt/buildhome/.nvm/versions/node/v12.8.0/lib/node_modules/@babel/core/node_modules/@babel/traverse/lib/context.js:112:16) 10:07:41 AM: at TraversalContext.visitSingle (/opt/buildhome/.nvm/versions/node/v12.8.0/lib/node_modules/@babel/core/node_modules/@babel/traverse/lib/context.js:84:19) 10:07:41 AM: at TraversalContext.visit (/opt/buildhome/.nvm/versions/node/v12.8.0/lib/node_modules/@babel/core/node_modules/@babel/traverse/lib/context.js:140:19) 10:07:41 AM: at Function.traverse.node (/opt/buildhome/.nvm/versions/node/v12.8.0/lib/node_modules/@babel/core/node_modules/@babel/traverse/lib/index.js:84:17) 10:07:41 AM: at NodePath.visit (/opt/buildhome/.nvm/versions/node/v12.8.0/lib/node_modules/@babel/core/node_modules/@babel/traverse/lib/path/context.js:97:18) 10:07:41 AM: Makefile.core.mk:49: recipe for target 'netlify' failed 10:07:41 AM: make: *** [netlify] Error 1 这是官方的一个 bug，已经解决。 CLA 检测不通过 如果你在设置 cla 之前提交了 PR，CI 里的 cla check 会失败。可以先在 PR 中回复 @googlebot I signed it.。如果还失败尝试回复 @googlebot I fixed it.。如果还不行，所以最好的办法是关闭当前 PR，重新用一个新的 branch 拷贝相应文件，再提交全新的 PR 即可。 ERROR: Unexpected end tag : p 如果遇到此错误，说明还没有完全修复 markdown 的 lint 问题。需要先修复完即可通过 CI 检查。 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/overview.html":{"url":"release/overview.html","title":"概述","summary":"本页面是对 Istio 历史版本变更追踪的概述。","keywords":"","body":"版本变更概述 历史版本追踪 下面是对 Istio 历史版本发布的追踪表格。 版本 发布时间 可用性 环境 API 性能 评论 0.1 2017-05-24 仅支持 HTTP，仅支持单 namespace，使用命令修改应用的 YAML 配置的方式注入 sidecar。 仅支持 Kubernetes 流量管理确立了RoutingRule、DestinationPolicy。 无 正式开源，该版本发布时仅一个命令行工具。确立了功能范围和 sidecar 部署模式，确立的 Envoy 作为默认 sidecar proxy 的地位。 0.2 2017-10-10 支持 TCP，支持自动注入 sidecar，支持自定义 mixer 扩展，支持自定义秘钥和证书给 Istio CA，支持 WebSocket 、MongoDB 和 Redis 协议。 支持 Nomad、Consul、Eureka、Cloud Foundry、Mesos，通过 Mesh expansion 的方式开始支持虚拟机 流量管理新增了EgressRule。 无 近五个月时间才发布了新版本，对于一个新兴的开源项目来说时间过长。 0.3 2017-11-29 无 无 无 无 无重大更新，主要承诺加快版本发布节奏为月度更新。 0.4 2017-12-18 支持使用 Helm Chart 安装。 增加了对 Cloud Foundry 平台的支持 无 无 距离上个版本发布仅 2 个多周，无重大更新，主要在平台和安装方式上增加了更多选项。 0.5 2018-02-02 支持渐渐式安装，支持使用 Kubernetes 的新特性实现 sidecar 自动注入。 无 无 无 主要增强易用性。 0.6 2018-03-08 Pilot 现在支持将自定义 Envoy 配置传递到 proxy。 无 无 无 常规更新，无重大变更。 0.7 2018-03-28 无 无 开始引入新的流量管理 API v1apha3，着手使用 VirtualService 和 DestinationRule 替换原先的 RoutingRule 和 DestinationPolicy。 无 主要改进测试质量。 0.8 2018-06-01 安全模块重命名为 citadel。 无 路由模型有重大调整，API 级别的重大更新，不再向前兼容。 无 变更之大堪称 1.0。 1.0 2018-07-31 支持多 Kubernetes 集群。 不再支持 Eureka、Cloud Foundry、Mesos。 API 更加稳定。 做了大量优化。 响应社区对 Istio 性能的质疑，优化了性能并出具了报告。虽然号称生产就绪，但是此时还没有充足的生产案例。 1.1 2019-03-19 新增配置管理组件 Galley，新增了 sidecar 资源，使用 Kiali 替换了 Istio 原先使用的 ServiceGraph 插件。 无 新增了 ServiceEntry。 在大企业中应用遇到瓶颈。 API 更加稳定，支持多 Kubernetes 集群，号称“企业就绪”。 1.2 2019-06-18 无 开始受主流云供应商支持。 无 无 主要改进发布机制，成立了多个与测试、发布相关的工作组。 1.3 2019-09-12 在安装时开始使用 manifest 文件。 无 无 无 常规更新，主要是优化用户体验。 1.4 2019-11-14 新增了 Istio Operator 的安装方式。 无 无 无 优化 Istio 的用户体验，提高 Istio 的性能。 1.5 2020-03-05 控制平面整合为 istiod，弃用 helm，使用 istioctl manifest 安装。 无 无 无 回归单体架构，支持 WebAssembly 扩展。 1.6 2020-05-21 进一步整合了 istiod，使用 istioctl install 命令来替代 manifest apply 的安装过程。 无 新增了 WorkloadEntry。 无 迈向极简主义，Istiod 更加完整，也彻底移除了 Citadel、Sidecar Injector 和 Galley。 1.7 2020-08-21 对 istioctl 命令进行了改进，增强易用性。 无 无 无 增强易用性。 1.8 2020-11-18 正式弃用 mixer，在 sidecar 中增加了智能 DNS 代理，重新回归到 helm 安装。 不再支持 consul 新增了 WorkloadGroup。 无 进一步完善了对虚拟机的支持。 1.9 2021-02-09 注重改善 Day2 体验。 无 无 无 没有重大功能，主要是稳定 API。 1.10 2021-05-19 继续改善 Day2 操作并改版了官网 无 新增了发现选择器。 无 通过引入发现选择器进一步优化了性能。 1.11 2021-08-12 无 无 无 无 CNI 插件取代了 istio-init 容器。 1.12 2021-11-19 无 无 新增了 WasmPlugin。 无 新增了 WASM 插件管理器。 重大版本 Istio 发展至今，历史上最重要的版本发布是： 0.1：正式开源 0.2：开始支持多种运行环境 0.8：API 重构 1.0：一个生产可用的临界点，此后 Istio 团队进行了大规模重组 1.1：支持多 Kubernetes 集群，并对性能进行了优化 1.5：重新回到单体，微服务组件合并成 istiod 1.8：正式废弃 Mixer，并重点增加对虚拟机的支持 更多历史等待我们书写…… 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/01.html":{"url":"release/01.html","title":"Istio 0.1——开启 Service Mesh 的新纪元","summary":"2017 年 5 月 24 日，Istio 正式开源。","keywords":"","body":"Istio 0.1——开启 Service Mesh 的新纪元 在用户将应用部署到 Kubernetes 上之后，如何管理容器之间的流量及确保应用的安全性，就成了突出问题，Service Mesh 的出现就是为了解决这一问题。在 Istio 开源之前，市场上只有创业公司 Buoyant 一家的 Service Mesh 产品 Linkerd，2017 年正值 Kubernetes 赢得容器编排之战，云原生社区中急需找到新的增长点，有人开始叫嚣“Kubernetes is becoming boring”，Service Mesh 开始抬头，Istio 的推出更使得该领域急剧升温。 2017 年 5 月 24 日，Google、IBM 和 Lyft 发布了 Istio 0.1。Istio 基于 Envoy 构建，在开源之初就确立的链接、保护、控制和观测”微服务”的使命。（注意，“微服务”后来在 Istio 的官网描述中被改成了服务，）该版本只支持 Kubernetes 环境，并计划在接下来的几个月添加诸如虚拟机和 Cloud Foundry 等环境的支持。计划每三个月发布一个大版本。 该版本发布时仅一个命令行工具 istioctl，但是它的意义是划时代的，它确立了 Service Mesh 的 sidecar 模式，即在应用容器 pod 中注入一个 proxy 来管理服务间通信，再通过控制平面统一管控这些 sidecar，后续的所有声称为 Service Mesh 的产品都应用了该模式。 参考 初次了解 Istio - istio.io 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/02.html":{"url":"release/02.html","title":"Istio 0.2——开始支持虚拟机","summary":"2017 年 10 月 10 日，Istio 0.2 发布，改善网格并支持多种环境。","keywords":"","body":"Istio 0.2——开始支持虚拟机 在等待了近 5 个月之久，Istio 0.2 版本终于面世。Istio 从 0.2 版本的发布开始支持虚拟机负载。初步支持将非 Kubernetes 服务（以 VM 或物理机的形式）添加到网格中。 这是此功能的早期版本，存在一些限制（例如，要求在容器和 VM 之间建立扁平网络）。 其他改进主要是关于可用性方面，例如支持 TCP，利用 Kubernetes 的 admission webhook（alpha 特性）支持自动注入 sidecar，支持自定义 mixer 扩展，支持自定义秘钥和证书给 Istio CA，支持 WebSocket 、MongoDB 和 Redis 协议。 参考 宣布 Istio 0.2——改善网格并支持多种环境 - istio.io 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/03.html":{"url":"release/03.html","title":"Istio 0.3——改进发布节奏","summary":"2017 年 11 月 29 日，Istio 0.3 发布，改进发布节奏。","keywords":"","body":"Istio 0.3——改进发布节奏 也许 Istio 团队意识到了从 0.1 到 0.2 版本的发布之间的时间跨度太久（近 5 个月），本次版本发布距离 0.2 版本仅 1 个多月时间，Istio 团队承诺接下来将每月发布一个版本，实际上在接下来的 8 个月里，Istio 一共发布了 6 个版本，基本实现了承诺。 本次版本发布并没有什么重大更新。 参考 Istio 0.3 发布公告——重大更新 - istio.io 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/04.html":{"url":"release/04.html","title":"Istio 0.4——新增平台支持","summary":"2017 年 12 月 18 日，Istio 0.4 发布，支持 Helm Chart 安装和 Cloud Foundry 平台。","keywords":"","body":"Istio 0.4——新增平台支持 距离上个版本发布仅 2 个多周，无重大更新，主要在平台和安装方式上增加了更多选项。支持 Helm Chart 安装和 Cloud Foundry 平台。 参考 Istio 0.4 发布公告——重大更新 - istio.io 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/05.html":{"url":"release/05.html","title":"Istio 0.5——对用户采用更友好","summary":"2018 年 2 月 2 日，Istio 0.5 发布，支持渐渐式安装，支持使用 Kubernetes 的新特性实现 sidecar 自动注入。","keywords":"","body":"Istio 0.5——对用户采用更友好 该版本主要增强易用性，相对于 Istio 初期一键安装所有组件的情况，现在 Istio 用户渐渐式采用，可以只安装 Istio 的部分组件。Istio 利用 Kubernetes 1.9 及以上版本的 muting webhook 特性，支持自动注入 sidecar。 参考 Istio 0.5 发布公告——重大更新 - istio.io 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/06.html":{"url":"release/06.html","title":"Istio 0.6——常规更新","summary":"2018 年 3 月 8 日，Istio 0.6 发布，该版本主要是常规更新，无重大变更。","keywords":"","body":"Istio 0.6——常规更新 该版本主要是常规更新，无重大变更。 参考 Istio 0.6 发布公告——重大更新 - istio.io 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/07.html":{"url":"release/07.html","title":"Istio 0.7——改进测试质量","summary":"2018 年 3 月 28 日，Istio 0.7 发布，主要提供测试质量。","keywords":"","body":"Istio 0.7——改进测试质量 该版本主要改进构建和测试基础架构并提高测试质量，并预告了 0.8 版本的重大更新，且是 API 层面的更新。 参考 Istio 0.7 发布公告——重大更新 - istio.io 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/08.html":{"url":"release/08.html","title":"Istio 0.8——1.0 的前奏","summary":"2018 年 6 月 1 日，Istio 0.8 发布，API 级别的变更，堪称 1.0。","keywords":"","body":"Istio 0.8——1.0 的前奏 该版本带来了重大的 API 级别的变更，新引进了 v1alpha3 路由 API，该 API 不向前兼容！确立了沿用至今的 Gateway（新引入，不在支持 ingress、egress 代理）、VirtualService（取代了原先的 RouteRule）、DestinationRule（取代了 DestinationPolicy）、ServiceEntry（取代了 EgressRule） 等资源类型。重命名了安全模块，以前的 Istio-Auth 或者 Istio-CA 现在被统称为 Citadel。 参考 Istio 0.8 发布公告——重大更新 - istio.io 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/10.html":{"url":"release/10.html","title":"Istio 1.0——生产就绪","summary":"2018 年 7 月 31 日，Istio 1.0 发布，API 更加稳定，支持多 Kubernetes 集群。","keywords":"","body":"Istio 1.0——生产就绪 2 个月前发布的 0.8 版本已经为 1.0 的发布做好大量的前置工作，经过这 2 个月的时间，0.8 版本的众多 alpha 功能现在变成了 beta，新增了支持将多个 Kubernetes 集群添加到单个 mesh 集群中。 在此之前社区对于 Istio 的性能质疑声不断，本版本对 Istio 的性能做了大量优化，虽然号称生产就绪，但是此时还没有充足的生产案例。 参考 Istio 1.0 发布公告——Service Mesh 生产就绪 - istio.io 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/11.html":{"url":"release/11.html","title":"Istio 1.1——企业就绪","summary":"2019 年 3 月 19 日，Istio 1.1 发布，API 更加稳定，支持多 Kubernetes 集群。","keywords":"","body":"Istio 1.1——企业就绪 距离 1.0 版本发布已经过去快 7 个月了，虽然越来越多的公司在生产中使用 Istio，但是一些大型公司在尝试使用 Istio 的过程中，遇到了一些瓶颈。此版本主要是优化性能，新增配置管理组件 Galley，新增了 sidecar 资源，可以更精细地控制附加到命名空间中工作负载的 sidecar 代理的行为。使用 RedHat 开发的 Kiali 替换了 Istio 原先使用的 ServiceGraph 插件。 参考 Istio 1.1 发布公告——Service Mesh 生产就绪 - istio.io 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/12.html":{"url":"release/12.html","title":"Istio 1.2——改进发布机制","summary":"2019 年 6 月 18 日，Istio 1.2 发布，常规发布，主要聚焦在测试和发布机制。","keywords":"","body":"Istio 1.2——改进发布机制 该版本属于常规发布，主要在测试和发布机制上进行了优化，重组了团队，新建了 GitHub Workflow、Source Organization、Testing Methodology 和 Build & Release Automation 团队，以方便度量每个团队的指标。 参考 Istio 1.2 发布公告——重大更新 - istio.io 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/13.html":{"url":"release/13.html","title":"Istio 1.3——改善用户体验","summary":"2019 年 9 月 12 日，Istio 1.3 发布，该版本主要聚焦在用户体验上。","keywords":"","body":"Istio 1.3——改善用户体验 Istio 1.3 主要着重于改善新用户使用 Istio 的体验，包括为 istioctl 增加更多调试功能，支持更多应用程序，无需任何其他配置。 参考 Istio 1.3 发布公告——重大更新 - istio.io 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/14.html":{"url":"release/14.html","title":"Istio 1.4——简化 Istio 的使用","summary":"2019 年 11 月 14 日，Istio 1.4 发布，该版本继续优化 Istio 的用户体验，提高 Istio 的性能。","keywords":"","body":"Istio 1.4——简化 Istio 的使用 常规发布， 继续致力于改善 Istio 的用户体验，并着重于简化使用方式，提高 Istio 的运行性能。 新增了 Istio Operator 的安装方式。 参考 Istio 1.4 发布公告——重大更新 - istio.io 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/15.html":{"url":"release/15.html","title":"Istio 1.5——拥抱变化，回归单体","summary":"2020 年 3 月 6 日，Istio 1.5 发布，有史以来最重大的一次变革。","keywords":"","body":"Istio 1.5——拥抱变化，回归单体 作者：马若飞，审校：罗广明 Istio 1.5 是一个具有重大变革的版本。长久以来，面对社区对 Istio 的性能和易用性的诟病，Istio 团队终于正视自身的问题，在当前版本中彻底推翻了原有控制平面的架构，完成了重建。正如 Simplified Istio 文中所说： 复杂是万恶之源，让我们停止焦虑，爱上单体。 Istio 1.5 回归单体，无论架构和使用方式都发生了巨大变化。因此笔者决定对 1.5 的变化内容做深入解读，以便开发者可以更好的理解和学习新版本，为使用和升级提供参考。 架构调整 这部分主要分析 Istio 1.5 在架构上的调整，这也是该版本最核心的变化。主要包括重建了控制平面，将原有的多个组件整合为一个单体结构 istiod；同时废弃了被诟病已久的 Mixer 组件。还对是否向后兼容的部分也做了说明，如果你要从 1.4.x 版本升级到 1.5 必须知道这些变化。 重建控制平面 官方使用的是重建（Restructuring）而不是重构（Refactoring）一词，可见其变化之大。在 Istio 1.5 中，控制平面将使用新的部署模式，将原有的各个组件整合在一起。 Istiod Istio 1.5 中会使用一个全新的部署模式：istiod。这个组件是控制平面的核心，负责处理配置、证书分发、sidecar 注入等各种功能。istiod 是新版本中最大的变化，以一个单体组件替代了原有的架构，在降低复杂度和维护难度的同时，也让易用性得到提升。需要注意的一点是，原有的多组件并不是被完全移除，而是在重构后以模块的形式整合在一起组成了 istiod。 Sidecar 注入 以前版本的 sidecar 注入是由 istio-sidecar-injector webhook 实现的。在新版本中 webhook 保留了下来，但整合进了 istiod 中，注入逻辑保持不变。 Galley 配置验证 - 功能保留，并入 istiod。 MCP Server - 改为默认关闭。对于大多数用户来说只是一个实现细节。如果确定依赖它，需要部署 istio-galley 并启动其进程。 实验特性（例如配置分析）- 也需要部署 istio-galley。 Citadel 以前 Citadel 的 2 个功能是生成证书以及 SDS 开启时以 gRPC 方式向 nodeagent 提供密钥。1.5 版本中密钥不再写入每个命名空间，只通过 gRPC 提供。这一功能也被并入 istiod。 SDS 节点代理 nodeagent 被移除。 Sidecar 以前，sidecar 能以两种方式访问证书：以文件挂载的密钥；SDS。新版本中所有密钥都存在本地运行的 SDS 服务器上。对绝大部分用户来说只需要从 istiod 中获取。对于自定义 CA 的用户，仍然可以挂载文件密钥，不过仍然由本地 SDS 服务器提供。这意味着证书轮询将不再需要 Envoy 重启。 CNI CNI 没有改变，仍在 istio-cni 中。 Pilot istio-pilot 的独立组件和进程被移除，由包含了它全部功能的 istiod 取而代之。为了向后兼容，仍有少许对 Pilot 的引用。 废弃 Mixer 在 Istio 1.5 中 Mixer 被废弃了。默认情况下 mixer 完全关闭。遥测的 V2 版本在新版本中是默认特性且不需要 mixer。如果你对 Mixer 的特殊功能有依赖，比如进程外适配器，需要重新开启 Mixer。Mixer 还会持续修复 bug 和安全漏洞直到 Istio 1.7 版本。mixer 的许多功能在 Mixer Deprecation 文档中都描述了替代方案，包括基于 Wasm sandbox API 的 in-proxy 扩展. 新版本中 HTTP 遥测默认基于 in-proxy Stats filter。这节省了 50% 的 CPU 使用量。1.5 中的遥测 V2 和老版本主要有以下几点不同： 流量的来源和目标如果没有注入 sidecar，部分遥测信息将无法收集。 Egress 遥测不再支持。 Histogram bucketization 和 V1 版本有很大不同。 TCP 遥测只支持 mTLS。 需要更多的 Prometheus 实例来收集所有代理的数据。 如果开发者之前使用的是 Istio 默认的 HTTP 遥测，迁移到新版本是没问题的。可以直接通过 istioctl upgrade 自动升级到 V2。 最被社区开发者唾弃的 Mixer 终于被废弃，可以说它是影响老版本性能的罪魁祸首。现在皆大欢喜，甚至呼声最高的 Wasm 方案也提上日程。当然我们也能看出 Istio 团队为了保证老版本的升级依赖并没有一刀切的干掉 Mixer，持续修复 bug 到 1.7 版本的深层含义是它会在 1.7 的时候被彻底移除？ 控制平面安全 老版本中，当设置了 values.global.controlPlaneSecurityEnabled=true 时，代理将安全地与控制平面交互，这是 1.4 版本的默认配置。每个控制平面组件都有一个带有 Citadel 证书的 sidecar，代理通过端口 15011 连接到 Pilot。 新版本中，不再推荐或以默认方式将代理连接到控制平面。作为替代，使用由 Kubernetes 或 Istiod 签发的 DNS 证书。代理通过端口 15012 连接到 Pilot。 功能更新 Istio 1.5 不仅仅做了减法，也做了很多加法，包括添加了新的功能，性能优化和 Bug 修复。这一部分列举了新版本中在流量管理、安全、遥测等多个功能方面的改进。 流量管理 提升了 ServiceEntry 的性能。 修复了 readiness 探针不一致问题。 通过定向局部更新的方式改善了配置更新的性能。 添加了为 host 设置所在负载均衡器设置的选项。 修复了 Pod 崩溃会触发过度配置推送的问题。 修复了应用调用自己的问题。 添加了使用 Istio CNI 时对 iptables 的探测。 添加了 consecutive_5xx 和 gateway_errors 作为离群值探测选项。 提升了 EnvoyFilter 匹配性能优化。 添加了对 HTTP_PROXY 协议的支持。 改进了 iptables 设置，默认使用 iptables-restore。 默认开启自动协议探测。 安全 添加 Beta 认证 API。新 API 分为 PeerAuthentication 和 RequestAuthenticaiton，面向工作负载。 添加认证策略，支持 deny 操作和语义排除。 Beta 版本默认开启自动 mTLS。 稳定版添加 SDS。 Node agent 和 Pilot agent 合并，移除了 Pod 安全策略的需要，提升了安全性。 合并 Citadel 证书发放功能到 Pilot。 支持 Kubernetes first-party-jwt 作为集群中 CSR 认证的备用 token。 通过 Istio Agent 向 Prometheus 提供密钥和证书。 支持 Citadel 提供证书给控制平面。 遥测 为 v2 版本的遥测添加 TCP 协议支持。 在指标和日志中支持添加 gRPC 响应状态码。 支持 Istio Canonical Service 改进 v2 遥测流程的稳定性。 为 v2 遥测的可配置性提供 alpha 级别的支持。 支持在 Envoy 节点的元数据中添加 AWS 平台的元数据。 更新了 Mixer 的 Stackdriver 适配器，以支持可配置的刷新间隔来跟踪数据。 支持对 Jaeger 插件的 headless 收集服务。 修复了 kubernetesenv 适配器以提供对名字中有.的 Pod 的支持。 改进了 Fluentd 适配器，在导出的时间戳中提供毫秒级输出。 Operator 用 IstioOperator API 替代了 IstioControlPlane API。 添加了 istioctl operator init 和 istioctl operator remove 命令。 添加缓存改善了调和速度。 性能和扩展性 为网关生成集群时忽略没用的服务。 为 headless 服务略过调用 updateEDS。 在 ingress 网关中默认关闭 SNI-DNAT 。 错误覆盖声明。 容量已知时，基于容量创建切片。 测试和发布 为 istioctl 创建了Docker镜像。 istioctl 添加 mTLS 分析器。 添加 JwtAnalyzer。 添加 ServiceAssociationAnalyzer。 添加 SercretAnalyaer。 添加 sidecar ImageAnalyzer。 添加 PortNameAnalyzer。 添加 Policy DeprecatedAnalyzer。 为 RequestAuthentication 添加了更多的验证规则。 istioctl analyze 从实验特性转为正式特性。 添加新标记 -A|--all-namespaces 给 istioctl analyze，来分析整个集群。 添加通过 stdin 到 istioctl analyze 的内容分析。 添加 istioctl analyze -L 显示所有可用分析列表。 添加从 istioctl analyze 抑制信息的能力。 为 istioctl analyze 添加结构化格式选项。 为 istioctl analyze 的输出添加对应的文档链接。 通过 Istio API 在分析器中提供标注方法。 istioctl analyze 可以基于目录加载文件。 istioctl analyze 尝试将消息与它们的源文件名关联。 istioctl analyze 可打印命名空间。 istioctl analyze 默认分析集群内资源。 修复分析器抑制集群级别资源消息的 bug。 为 istioctl manifest 添加多文件支持。 替换 IstioControlPlane API 为 IstioOperator API。 为 istioctl dashboard 添加选择器. 为 istioctl manifest --set 标记添加切片和列表支持。 总结 Istio 1.5 是全面拥抱变化的一个版本。重建整个控制平面，打造了全新的部署模式 istiod；摒弃了拖累系统性能的 Mixer；保证兼容性也不忘持续优化和引入新的功能。在彻底抛弃历史包袱的同时，Istio团队也用他们的勇气践行了敏捷开发的真谛。随着稳定的季度发布，相信未来的 Istio 会越加成熟。让我们拭目以待。 参考 Istio 1.5 发布公告——重大更新 - istio.io 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/16.html":{"url":"release/16.html","title":"Istio 1.6——迈向极简主义","summary":"2020 年 5 月 12 日，Istio 1.6 发布，迈向极简主义。","keywords":"","body":"Istio 1.6——迈向极简主义 作者：马若飞，审校：罗广明 从 1.2 版本开始，Istio 进入季度发布的节奏。5 月 21 日发布的 1.6 版本可以说是最准时的一次。我们是否可以理解 Istio 架构简化后的开发工作已经步入了正轨？这次的更新是否会带给我们惊喜？亦或是还有遗憾？让我们一一道来。 加法和减法 Istio 1.6 的 Release note 开篇的标题用三个巨大的 Simplify 来表明态度：我们要把极简主义进行到底！其中最大的简化就是将原有组件的功能完全整合入 Istiod ，完成了悟天克斯们的合体过程，让 Istiod 更加完整，也彻底移除了Citadel、Sidecar Injector 和 Galley。当然，你也可以理解为，这其实是对 1.5 版本未完成工作的收尾。 图片引自鸟山明漫画《龙珠Z》 第二项简化工作是添加 istioctl install 命令来替代 manifest apply 的安装过程，用更直观、更精简的命令改善安装过程的体验。当然，manifest 子命令依然保留，你还是可以通过清单方式进行部署。在 Change Notes 的三十多项更新中，有七个是removed，基本上都和安装有关，比如移除已经废弃的 Helm charts、istio-pilot的配置等。可以看出，Istio 团队在极力的通过优化安装流程和部署形态来提升用户的体验。互联网产品有一个很重要的指标叫留存率，安装过程导致的用户流失是非常不值得的，需要花大力气进行流程的优化和调整。毕竟，第一印象的重要性毋庸置疑，以一个干练清爽的年轻人形象去相亲，还是扮演一个拖泥带水的油腻大叔？成功率高下立判。看来 Istio 团队终于醍醐灌顶，要努力做一个干练的奶油小生了。 再来说说加法。Change Note 中的新增项主要来自四个方面：虚拟机的支持，遥测（Telemetry）的改进，升级，istioctl 命令行。 Istio 通过添加了一个叫 WorkloadEntry 的自定义资源完成了对虚拟机的支持。它可以将非 Kubernetes 的工作负载添加到网格中，这使得你有能力为 VM 定义和 Pod 同级的 Service。而在以前，你不得不通过 ServiceEntry 里的 address 等字段，以曲线救国的方式去实现对非 Pod 工作负载的支持，丑陋又低效。WorkloadEntry 的引入让非 Kubernetes 服务接入网格成为现实。 apiVersion: networking.istio.io/v1alpha3 kind: WorkloadEntry metadata: name: details-svc spec: serviceAccount: details-legacy address: vm1.vpc01.corp.net labels: app: details-legacy instance-id: vm1 遥测方面，增加了两个实验性的功能，一个是请求类别过滤器，主要用来对不同 API 方法的请求进行标记和分类；另一个是追踪配置API，可以控制采用率等。除此之前，添加了 Prometheus 标准的抓取标注（annotation），提升了集成的体验。Grafana 的 Dashboard 也有更新，对终端用户来说这倒是可以期待一下。然而，我们最关心的 WASM 只字未提！笔者猜测它在可用性方面还有很多问题。ServiceMesher社区有成员对 Istio 各个版本的遥测做了 benchmark，横向对比的结果是 WASM 方式下性能垫底！甚至还不如 1.3 版本。这让人不禁感慨，WASM 之于 Envoy，会不会只是一次看上去很美好的邂逅呢？ 图片引自几米漫画《向左走向右走》 为了能让升级过程更平滑，Istio 提供了一种叫金丝雀升级（Canary upgrades）的策略，可以安装一个金丝雀版本的控制平面，与老版本同时存在。金丝雀升级可以让你逐渐的切换流量以验证新版本的可用性，同时也保留了回滚的能力。当然，如果你足够自信，依然可以原地升级（In place upgrade)。 $ istioctl install --set revision=canary $ kubectl get pods -n istio-system NAME READY STATUS RESTARTS AGE istiod-786779888b-p9s5n 1/1 Running 0 114m istiod-canary-6956db645c-vwhsk 1/1 Running 0 1m 令笔者惊奇的是，Istio 居然提供了降级（Downgrade）功能！这是因为开发团队对新版本不自信吗？如果说金丝雀升级已经提供了回滚的能力，那又何必为原地升级提供降级能力呢？而且降级也是有限制条件的：必须是 1.5 以上的版本；必须是使用 istioctl安装的；必须使用老版本的 istioctl 执行降级操作。笔者很难想象出它的适用场景，如果你知道，请一定告知我。 向市场妥协 在虚拟机支持方面，Release Note 中有这样一句话： Expanding our support for workloads not running in Kubernetes was one of the our major areas of investment for 2020 Istio 为什么要花大力气支持 VM 这种即将过气的部署载体？而且要作为 2020 年开发的重中之重？在理解这一举措之前，让我们先来看看 Google 的老对手 Amazon，在自家产品 AWS App Mesh 上的布局。 从下图可以看出，AWS App Mesh 支持了自家的各种工作负载，当然也包括虚拟机和实体机。Amazon CTO Werner Vogers 在使用 AWS App Mesh 重新定义服务通信 一文中写到： Our goal is to remove the undifferentiated heavy lifting of operating complex applications. We provide the tools, services, and observability to ensure that you can maintain high standards for your own architectures. appmesh 这种全方位部署形态的支持，其主要目的就是消除不同，给用户统一的接入体验。而这种不得已为之的策略，其实本质上是云原生应用落地的现状造成的。如果你多了解下身边不同公司的情况，你就会发现现实远比我们想象的要骨感。大量的中小企业都刚刚把上云提上日程，还有很多是实体机 / VM 和部分迁移到云的业务组成的混合体，其部署形态的复杂性可想而知。而 App Mesh 这种远瞻性的策略就是通过覆盖用户现有的各种部署形态，提前将未来全面上云的市场揽入怀中。当你的网格接入层无需变更时，只需要将 VM 之类的负载一一替换即可，最终完成云上的纯粹形态。 我们再来看看 Istio 的同门师弟，Google Cloud Traffic Director 的情况。其官方文档中有如下描述： 按您的节奏进行现代化改造 Traffic Director 既适用于基于虚拟机 (Compute Engine) 的应用，也适用于容器化应用（Google Kubernetes Engine 或自行管理的 Kubernetes），并能分阶段逐步运用于您的服务。 看到这里我想你应该很清楚了，两大云厂商的网格产品无一例外地选择对 VM 支持，主要原因就是综合 Kubernetes、Service Mesh 落地现状，以及市场策略的考量。Istio 选择跟进无可厚非，且可以和自家产品互取所长，共享资源。我们是否可以大胆猜测一下：未来 Istio 可能会和 Traffic Director 兼容甚至合并？ 1.6 版本的最大变化，就是提供对虚拟机的支持，也是本年度的重点，其原因我们梳理如下： 基于云原生落地现状的考虑：无论是 Kubernetes、Service Mesh，还是云迁移，整个业界依然处于参差不齐的状态，部署形态复杂多变。对 VM 的支持可以为用户提供统一的接入体验，并平滑接入网格技术。 对抗（蚕食）竞争对手的市场：在收费的云平台，两大高手的策略旗鼓相当；而在开源方面，Istio 无疑是绝对的主角。同时在付费和免费两个层面统一战线，Istio 的这一神补刀，可以说击中了 AWS 的要害，这可能让本来摇摆不定的技术选型者改变主意。 回归平台中立的理念：Istio 一经推出就宣称具有多平台支持的能力，但两年下来大家都心照不宣，知道它对 Kubernetes 的强依赖性。提供 VM 支持正是去平台化的良机，为自己曾经立的 Flag 正名。 另一个很可能属于市场宣传的行为就是高调宣称支持 Kubernetes Service APIs。对于一个还处于 alpha 版本的功能，如此大力地进行支持，再想想 Istio 与 Kubernetes 网络组的关系，不得不让人感觉有广告嫌疑。都是老熟人，互推一下也是应该的。不过亮相的略显刺眼，些许尴尬。 第三个值得一说的是：开始提供新特性预览。 Added a new profile, called preview, allowing users to try out new experimental features that include WASM enabled telemetry v2. 笔者看到这一项的第一反应：这不就是 AWS Preview 吗？AWS 通过这种方式把新功能提前释放给用户使用，以收集使用数据和建议，来改进功能，可以认为是一种 alpha 测试。Istio 的这一举动可以理解为终于要开始践行 MVP（最小化可行产品）理论了，毕竟从前脱离市场和用户，闭门造车的跟头栽得有点大，痛定思痛，终于选择和用户站在一边。 重要的生态圈 就在 1.6 发布不久前，一直致力于 Service Mesh 生态链产品开发的 solo.io 公司推出了第一个 Istio 开发者门户（Developer portal）。它可以对网格中运行的 API 进行分类，通过 webUI 提供 API 管理的可视化用户体验，同时还能自动地生成 Istio Gateway、VirtualService 这些自定义资源。 portal solo.io 和 Google Cloud 是商业伙伴关系，旗下的产品 Gloo 和 Service Mesh Hub 都已经整合进了 GCP 和 GKE 中。而这一次发布 Portal 无疑又是一次双赢。 Service Mesh 目前的市场格局并不明朗，依然是硝烟弥漫，产品都尚未定型，更不要说生态圈了。对 Goolge 而言，能提前将 solo.io 这样有实力的小弟招入麾下，围绕 Istio 打造一整套生态链产品，无疑会让 Istio 如虎添翼，也极有可能在市场竞争中增加重量级的砝码。 而对于 solo.io 而言，作为初创公司，能和 Envoy 一样抱着 Service Mesh 头号网红的粗腿，既能持续地获得大量的曝光机会，又能为自己的产品带来持续增长的用户群体。这又让我想起在当年在游戏行业的一对CP，Facebook 和 Zynga。Facebook 为 Zynga 输送了大量的用户，而 Zynga 的社交游戏又为 Facebook 的用户留存和黏性做出了贡献。至于后来的相爱相杀，那又是另外的故事了。我们回归正题，别说是 solo，换做是笔者本人，估计早都高喊着“土豪求做友”跪舔着不放手了。无论如何，solo.io 的一步妙棋，很可能会搅动整个棋局的变化，让我们拭目以待。 生态圈的重要性无需多言，即便如 AWS 这种闭环生态的巨鳄，每年 Summit 也会把最大的展台留给众多 vendor，所谓一个好汉三个帮，谁都不会拒绝有实力的伙伴补强你的实力，玩 LOL 的的朋友会感慨，辅助是多么的重要！Java 叱咤风云二十五年，全靠以 Spring 为首的大将们强力补刀；Golang 若不是没有一个强大的、统一的标准化的生态，估计早把 Java 按在地上摩擦了。再反观日本的动漫产业，再优秀的作品，如果没有丰富的周边和产业链，GDP 要小上几个数量级，恐怕二次元的小弟弟小妹妹们都会少了很多精神寄托吧。 期许和无奈 在 InfoQ 最新发布的“技术采用生命周期”调查报告中，将 Istio、Service Mesh 放入了早期采用者这一列，这其实也从应用和市场层面反映出了它们仍处在产品发展的前期，任重道远。产品有限的成熟度制约了技术选型的空间，反过来市场的谨小慎微又让产品缺少了来自实践端的经验和积累。 infoq 本次 1.6 版本的发布我个人认为是惊喜不足，失望有余。Mixer 的寿终正寝让中心化的限流、黑白名单这样有用的功能也跟着陪了葬，还未看到要弥补它们的打算。而期待的 Envoy 与 WebAssembly 的强强联手也在本次更新中只字未提。当然了，产品迭代哪能一蹴而就，时间、成本、质量三要素缺一不可。一统江山的 Kubernetes 到了 1.8 才算是稳定版本，何况现在的 1.18，而我们的 Istio才刚刚到 1.6 而已，还需要更多的时间来沉淀。 前路漫长，但仍可期许，让我们未来见分晓。 参考 Istio 1.6 发布公告——重大更新 - istio.io 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/17.html":{"url":"release/17.html","title":"Istio 1.7——进击的追风少年","summary":"2020 年 8 月 21 日，Istio 1.7 发布，依然还是个少年。","keywords":"","body":"Istio 1.7——进击的追风少年 作者：马若飞，审校：宋净超 2020 年 8 月 21 日，Istio 发布了 1.7 版本。除了介绍新版本的主要更新内容外，本文会重点分析 Istio 团队在产品更新策略上的激进态度和举措。是稳扎稳打做好向后兼容，带给用户所承诺的易用性；还是快刀斩乱麻，做进击的追风少年，且听笔者慢慢道来。 如约而至——Istio 1.7.0 发布 就在几天前，Istio 发布了 1.7 版本，和 1.6 版本的发布时间正好间隔三个月，完美的实现了季度发布的诺言。本次发布的口号是 “伟大的 Istio 社区（Istio’s great community）”，因为有来自 40 多个公司的 200 多个开发者做出了贡献。Istio 官方是这样描述的： 正是因为有如此令人惊羡（amazing）的社区，才让 Istio 能够在每个季度有如此多的改进。 Istio 团队已经从上个月倒卖商标的麻烦中走了出来，看上去是想通过强调 Istio's great community 这个理念来抚平社区开发者受伤的心灵？笔者认为，作为开发者和用户不必太在意 Google 的商业行为，至少现阶段 Istio 还在以开源的身份持续演进，还能为我所用，这就足够了。 1.7 版本中重要的更新主要有以下四个方面。 安全增强 确认了使用安全发现服务（SDS）作为证书分发的优势，并把它作为一个重要的安全最佳实践。现在这一特性也被使用在出口网关上。 信任域验证除了支持 HTTP，现在也可以验证 TCP 流量了，并且还支持在 MeshConfig 中进行配置，提供了更多灵活性。 可以使用 ECC 进行 CA 通信，提高了安全性和效率。 网关默认使用非根（non-root）用户部署，这主要是基于一条最佳实践：不要让运行的进程有多于它所需的权限，这会导致不必要的混淆。 提升易用性 在易用性方面主要的改进依然是对 istioctl 命令行工具的增强： analysis 支持 对可能不安全的 DestinationRule 配置发出警告 对使用废弃的 Mixer 资源发出警告 可以使用 ISTIOCONFIG 设置自定义配置 使用助记符来标识端口号 添加了 istioctl x uninstall 来方便卸载 Istio 生产运维改进 在运维方面也有些许改进，例如： 可以支持让 Sidecar 启动后才启动你的应用容器。如果你的应用需要在启动时通过 Sidecar 代理来访问资源，这项修改可以让部署变的更稳定（避免因为 Sidecar 没启动而应用访问不到资源的情况）。 Istio Operator 作为最佳安装方式。Operator 在之前的版本就已经提供了，看上去 Istio 想主推 Operator 以替代其他的安装形式。但笔者必须要吐槽一下官方发布文档对这一条的描述： The Istio Operator is a great way to install Istio, as it automates a fair amount of toil. Canary control plane deployments are also important; they allow ultra-safe upgrades of Istio. Unfortunately, you couldn’t use them together - until now. 吹了一大堆，其实翻译成人话就是：Operator 目前还不支持金丝雀更新。真是佩服这段文案编写者拐弯抹角的能力。 提供了 istio-agent 的指标，可以观察它的运行情况 Prometheus 指标收集方面的改进 VM 安全性 持续对虚拟机相关功能的开发是本年度的重点，这是 Istio 多次强调的。这是因为目前客户应用部署环境的复杂性和混合性，VM 依然是一种主要的部署选择。和一些托管的竞品（比如 AWS APP Mesh ）相比，Istio 缺失了这方面的能力，使得这些客户不得不观望而无法落地。对 VM 的支持就成为了重中之重，这也是商业上的考量。 然而本次更新没有太多的重量级功能发布，只是做了小的改进，且还在 alpha 阶段。比如为 VM 也增加了安全特性，支持证书自动轮转；istioctl 现在可以验证 VM 的代理状态；增加了 RPM 安装包等。 温柔一刀——升级的伤痛 客观的讲，以上官方的发布文档大部分内容都不痛不痒，对使用层面的用户影响不大。而真正和用户息息相关是安装和升级的变化。Istio 团队并没有在发布首页强调这一点，这引起了笔者的强烈不适并严重怀疑 Istio 有刻意规避问题的嫌疑。我们先来看笔者认为最重要的一条变更： 过分严格的平台版本限制 Require Kubernetes 1.16+ Kubernetes 1.16+ is now required for installation. 这是 Istio 官方第一次在新版本的 Release Note 中明确的说明了 Kubernetes 的版本限制问题。尽管以前老版本的 Istio 也会对平台版本有要求，但通常是这样的口吻： Istio 1.5 has been tested with these Kubernetes releases: 1.14, 1.15, 1.16. 这种描述隐含的意思就是：我们在这几个版本测试过兼容性，但我们并没有说 Istio 不兼容其他版本，可能、也许、大概是兼容的，我们只是没有测试过而已。而这一次是描述是 “required”，请仔细体会这两种说法的区别。 为了验证 1.7 真实的兼容性（ required 只是骇人听闻？），笔者做了一次安装测试，测试环境为 Docker 桌面版内置的 Kubernetes，版本 v1.15.5。 首先，使用预检命令验证集群环境是否合法（新版本已经取消了 istioctl verify-install 命令） $ bin/istioctl x precheck Error: 1 error occurred: * The Kubernetes API version: v1.15.5 is lower than the minimum version: 1.16 果然，预检没有通过，出现了版本过低的错误。笔者忽略预检结果，尝试强行安装，想看看预检是否也只是吓唬人而已： $ bin/istioctl install This will install the default Istio profile into the cluster. Proceed? (y/N) y The Kubernetes version v1.15.5 is not supported by Istio 1.7.0. The minimum supported Kubernetes version is 1.16. Proceeding with the installation, but you might experience problems. See https://istio.io/latest/docs/setup/platform-setup/ for a list of supported versions. ✘ Istio core encountered an error: failed to wait for resource: failed to verify CRD creation: the server could not find the requested resource 验证结果被现实啪啪打脸。除了对版本限制的说明，Istio 还非常严谨的告知安装过程会继续，但你可能会遇到各种问题。果然，在 Istio core 的安装步骤中就报了错，安装过程被卡住无法继续进行。看来这一次 Istio 的 required 是来真的了。 为什么说这个强制性的版本限制会对用户造成最大的困扰？其根本原因就是当前绝大部分企业和用户所使用的 Kubernetes 根本没有达到 1.16+ 版本，大部分都是基于 1.14、1.12，甚至更低。目前两大云厂商的 Kubernetes 服务（AWS EKS 和 GCloud GKE）也都是兼容 1.14+，这也能从一个侧面说明有一大批老用户很可能都使用的是 1.14 版本。然而 Istio 并没有遵循这一规则，这等于直接将很大一部分用户踢出了场外，Istio 1.7 不带你们玩了。 另一个潜在的问题是为想要升级的用户带来了极大的困惑。举一个例子：某企业的运维团队正在打算将 1.14 版本的 Kubernetes 升级到 1.16，而架构团队正打算将安装在其上的 Istio 1.2 升级到 1.7。这个团队所面临的问题是，要升级到 Istio 1.7 必须先升级 Kubernetes 到 1.16；但是一旦升级了 1.16，原本的 1.2 版本很可能有兼容问题，因为 Istio 1.2 宣称只在 Kubernetes 1.12~1.14 测试过。Istio 1.7 过分严格的的平台版本限制给了这些用户致命一刀，升级之路充满荆棘。他们只能退而求其次选择老版本进行升级。 从 1.5 版本开始，Istio 一方面不断的强调易用性和用户体验，一方面又武断的放弃向下兼容，将大量用户拒之门外。其自相矛盾的行为令人匪夷所思。 资源版本号的变更 这一问题出现在 Change Note 安装部分的一条，很可能成为升级用户新的痛点。 Upgraded the CRD and Webhook versions to v1. (Issue #18771),(Issue #18838) 从 Issue 可以看出，因为 Kubernetes 在 1.16 中将 webhook 的 API 版本改为 v1，并会在 1.19 版本中删除老的 v1beta 版本。这一激进行为导致 Istio 不得不在自己的 1.8 版本之前完成对应的迁移。笔者在 Istio 官方 Slack 中也验证了这一问题： Yes this is a hard requirement. Most specifically CRDs, and other apis use APIs that were promoted to v1 in 1.16 are being used. Istio 开发团队也在 Issue 中抱怨对方太激进（aggressive），留给他们的开发周期太短（pretty tight window），有很多工作要做（probably a lot of work），一副巧妇难为无米之炊的委屈样。笔者不由得感叹：本是同门师兄弟，相煎太急！ 而对于用户而言，意味着你不得不将自己的 mesh 配置文件的版本号进行更新，如果集群比较庞大，很可能有不少的工作量（主要是测试、验证方面）。你很可能还需要通过金丝雀升级的方式进行，因为无论是先升级 Istio，还是先修改配置，都可能出现兼容问题（说好的易用性和用户体验呢？）。 短暂的 LTS 在 Istio 的版本支持公告页面，你可以发现以前的老版本都逐渐的停止了维护，特别是具有里程碑意义的 1.5 版本，在发布 6 个月后即停止维护，几乎成为了 Istio 史上最短命的版本。Istio 在构建和发布节奏页面中这样定义 LTS（long term support）： Support is provided until 3 months after the next LTS 即上一个版本会在新版本发布后的 3 个月就停止维护（包括更新、修复 bug 等），算上它自己的发布日期，也只有半年时间。我们再来对比一下 Ubuntu 对 LTS 的定义，下面是 Ubuntu 20.04 LTS 的一段说明： 下载专为桌面 PC 和笔记本精心打造的 Ubuntu 长期支持 (LTS) 版本。LTS 意为 “长期支持”，一般为 5 年。LTS 版本将提供免费安全和维护更新至 2025 年 4 月。 5 年对 3 个月。对于操作系统来说，因为处在整个软件架构的最底层，理应保证长期稳定的维护。Service Mesh 比不了操作系统，但好歹也是基础设施，也应该对上层建筑提供更多稳定性。这个所谓的长期是不是有点过于短暂？追风少年你是要赶着去投胎吗？Istio 对 LTS 的定义让我开始怀疑人生。 路在何方——稳定是永恒的童话？ Service Mesh 领域的权威人士 Christian Posta 在公开采访中表示：Istio 1.7 将会是真正意义上的稳定、可用于生产环境的版本。笔者对此不敢苟同。本次更新表现平平，并无亮点，反倒是对 Kubernetes 的版本限制会导致用户在安装、升级环节增加成本和不确定性，是一次用户体验上的倒退。Istio 1.0 版本就宣称是生成环境可用（Production ready），恐怕这一次也依然会变成川建国金句的翻版：Make Istio production ready again! 经过了 3 年多的迭代，Istio 依然像个毛头小子，随性而为。稳定和可靠，在这里成了骗人的童话故事。笔者曾分析 Istio 1.8 将会是第一个稳定版本，希望下一次不要让我们失望。 参考 Istio 1.7 发布公告——重大更新 - istio.io 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/18.html":{"url":"release/18.html","title":"Istio 1.8——用户至上的选择","summary":"2020 年 11 月 18 日，Istio 1.8 发布，顺应用户呼声，对虚拟机的支持更进一步。","keywords":"","body":"Istio 1.8——用户至上的选择 作者：宋净超，审校：马若飞 今天 Istio 1.8 发布了，这是 Istio 在 2020 年发布的最后一个版本，按照 Istio 社区在今年初设定的目标继续推进，该版本主要有以下更新： 支持使用 Helm 3 进行安装和升级 正式移除了 Mixer 新增了 Istio DNS proxy，透明地拦截应用程序的 DNS 查询，实现智能应答 新增了 WorkloadGroup 以简化对虚拟机的引入 WorkloadGroup是一个新的 API 对象，旨在与虚拟机等非 Kubernetes 工作负载一起使用，模仿现有的用于 Kubernetes 工作负载的 sidecar 注入和部署规范模型来引导 Istio 代理。 安装与升级 Istio 从 1.5 版本开始弃用了 Helm，使用 istioctl manifest 方式安装，后来又改成了 istioctl install，现在又重新回归了 Helm，Helm 作为 Kubernetes 环境下最常用的应用安装管理组件，此次回归也是倾听用户声音，优化安装体验的的反应吧，不过 Istio Operator 依然将是 Istio 安装的最终形式，从 1.8 版本开始 Istio 支持使用 Helm 进行 in-place 升级和 canary 升级。 增强 Istio 的易用性 istioctl 命令行工具新的了 bug reporting 功能（istioctl bug-report），可以用来收集调试信息和获取集群状态。 安装 add-on 的方式变了，在 1.7 中已经不推荐使用 istioctl 来安装，在 1.8 中直接被移除了，这样有利于解决 add-on 落后于上游及难以维护的问题。 正式移除了 Mixer，推荐使用 WebAssembly 通过扩展 Envoy 的方式来扩展 Istio，也推荐大家使用 GetEnvoy Toolkit 来进行 Envoy 的扩展开发。 对虚拟机的支持 在我之前的博客中谈到 Istio 1.7 如何支持虚拟机，在 Istio 1.8 中新增了智能 DNS 代理，它是由 Go 编写的 Istio sidecar 代理，sidecar 上的 Istio agent 将附带一个由 Istiod 动态编程的缓存 DNS 代理。来自应用程序的 DNS 查询会被 pod 或 VM 中的 Istio 代理透明地拦截和服务，该代理会智能地响应 DNS 查询请求，可以实现虚拟机到服务网格的无缝多集群访问。 新增了 WorkloadGroup ，它描述了工作负载实例的集合。提供了一个规范，工作负载实例可以用来引导它们的代理，包括元数据和身份。它只打算与虚拟机等非 Kubernetes 工作负载一起使用，旨在模仿现有的用于 Kubernetes 工作负载的sidecar注入和部署规范模型来引导 Istio 代理。 在 Tetrate，我们在客户的多集群部署中广泛使用这种机制，以使 sidecar 能够为暴露在网格中所有集群的入口网关的主机解析 DNS，并通过 mTLS 访问。 总结 总而言之，Istio 团队履行了年初的承诺，自 2018 年发布 1.1 版本发布起，保持了固定的发布节奏，每 3 个月发布一个版本，在性能、用户体验上持续优化，以满足 brownfiled 应用与 greenfield 应用在 Istio 上的无缝体验。我们期待 Istio 在 2021 年可以给我们带来更多惊喜。 参考 Istio 1.8 发布公告——重大更新 - istio.io 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/19.html":{"url":"release/19.html","title":"Istio 1.9——提升 Day2 体验","summary":"2021 年 2 月 9 日，Istio 1.9 发布，重点提升 Day2 体验。","keywords":"","body":"Istio 1.9——提升 Day2 体验 2 月 9 日，Istio 宣布发布 Istio 1.9 版本。在这个版本中，我们可以看到虚拟机更广泛地被采用到服务服务网格中，而且对虚拟机的支持、对虚拟机的 cert 发放、对工作负载入口的健康检查也更加完善。Istio 最新的 1.7 和 1.8 两个版本，在让 VM 成为服务网格中一流的工作负载方面取得了很多进展，而 cert 发放则是最后需要弥补的缺口。 服务网格中的虚拟机集成 虚拟机集成是 Istio 的核心功能之一，在这个版本中，它升级到了测试版，这意味着它可以用于生产，不再是玩具。 在 Istio 服务网格中运行 Kubernetes 工作负载已经有一段时间了，在过去的几个 Istio 版本中，运行虚拟机工作负载也是如此。最新发布的 Istio 使得在服务服务网格中混合 Kubernetes 和 VM 工作负载更加容易。 常见的用例包括在数据中心的虚拟机或云环境中的虚拟机上运行应用程序。这些虚拟机要么运行传统的，要么运行第三方应用 / 服务。其中一些应用 / 服务不会在短时间内消失–或者在某些情况下，永远不会消失！其中一些虚拟机工作负载是应用现代化历程的一部分，包括转向微服务或 RESTful 服务，部署为分布式服务，其中一些在容器中运行。在这个应用现代化历程中，其中一些虚拟机运行单体工作负载，直到它们被分解为微服务：在虚拟机中运行这些应用提供了一条通往目标 RESTful 服务或 API 的路径，并使过渡更加平稳。 通过这样的渐进式方法，您可以开始将运行在虚拟机中的现有应用程序上岗到服务网格中。然后，随着你建立起你的服务服务网格实践，你可以逐渐将这些单体应用分解为服务，并更轻松地将它们部署在多个集群、云和混合环境中。Istio 可以使用 WorkloadEntry、WorkloadSelector 和 WorkloadGroup 来帮助您实现这一点，管理服务网格中的虚拟机，以促进您的应用现代化历程中更有保障的过渡。 与 Kubernetes Service API 保持一致 通过 Kubernetes 服务 API，基础设施提供商和平台运营商可以为不同的目的设置多个 Controller。因此，它将 Gateway 与 Envoy 解耦，方便了 Istio 中不同反向代理后端的使用。 Istio 从 1.6 版本开始就积极与 Kubernetes SIG-NETWORK 组合作，使用 Kubernetes Gateway API 来替代现有的 Gateway 声明，并将服务网格中的服务对外暴露。以前，你需要创建一个 VirtualService 来绑定到 Gateway 上，以便将服务暴露在服务网格之外。现在，您可以使用 GatewayClass、Gateway 和 Route。GatewayClass 定义了一组共享共同配置和行为的 Gateways。这类似于 IngressClass 的 Ingress 和 StorageClass 的 PersistentVolumes。Route 类似于 VirtualService 中的 Route 配置。你可以参考 Istio 文档来尝试这个功能，但要注意这个功能还处于实验阶段。 资源关系 总结 Istio 1.9 让每个功能的状态更加清晰，这也有助于增强用户使用的信心。经过最近几次大的改动，相信 Istio 的 API 会在进一步的发展中变得更加稳定。 将服务服务网格扩展到虚拟机，一直是 Tetrate 成立的重要使命之一。Tetrate 提供 Istio 支持，以及为多集群、多租户和多云构建的基于 Istio 的优质服务网状管理平台。 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/110.html":{"url":"release/110.html","title":"Istio 1.10——继续改善 Day2 操作并改版了官网","summary":"2021 年 5 月 19 日，Istio 1.10 发布，继续改善 Day 2 操作，并改版了官网。","keywords":"","body":"Istio 1.10——继续改善 Day2 操作并改版了官网 北京时间 5 月 19 日，我们很高兴地宣布 Istio 1.10 的发布！我们要特别感谢我们的发布经理 Sam Naser 和 张之晗，以及整个测试和发布工作组在 1.10 中的工作。 这是我们 2021 年的第二个版本，和过去几个版本一样，我们继续为 Istio 用户改善 Day 2 操作。 该版本的亮点如下。 发现选择器 在以前的 Istio 版本中，Istio 的控制平面一直在观察和处理集群中它所关心的所有 Kubernetes 资源的更新。这在大型集群或配置快速变化的集群中可能是一个可扩展性瓶颈。发现选择器（Discovery Selector）限制了 Istiod 监视的资源集，所以你可以很容易地忽略那些与网格无关的命名空间的变化（例如一组 Spark Job）。 你可以认为它们有点像 Istio 的 Sidecar API 资源，但对于 Istiod 本身来说：Sidecar 资源限制了 Istiod 将发送至 Envoy 的配置集。发现选择器限制了 Istio 从 Kubernetes 接收和处理的配置集。 请看 Lin、Christian 和 Harvey 的精彩文章，深入了解这项新功能的情况。 稳定的修订版标签 早在 1.6 版本中，Istio 就增加了对安全部署多个控制平面的支持，并且我们一直在稳步提高支持度。关于修订版的一个主要的可用性抱怨是需要大量的命名空间重新标记来改变修订版（revision），因为一个标签（label）直接映射到一个特定的 Istio 控制平面部署。 有了修订版标签，现在有了一个间接层：你可以创建像 canary 和 prod 这样的标签，把使用这些标签的命名空间标记为修订版（即 istio.io/rev=prod），并把特定的 Istiod 修订版与该标签联系起来。 例如，假设你有两个修订版，1-7-6 和 1-8-0。你创建一个指向 1-7-6 版本的修订标签 prod，并创建一个指向较新的 1-8-0 版本的修订标签 canary 命名空间 A 和 B 指向 1-7-6，命名空间 C 指向 1-8-0。 现在，当你准备将 1-8-0 修订版从 canary 推到 prod 时，你可以将 prod 标签与 1-8-0 Istiod 修订版重新关联。现在，所有使用 istio.io/rev=prod 的命名空间将使用较新的 1-8-0 版本进行注入。 命名空间 A、B 和 C 指向 1-8-0 请查看更新后的 Canary 升级指南。 Sidecar 网络变化 在以前的 Istio 版本中，Istio 已经重写了 pod 网络，从 eth0 捕获流量，并将其发送到 lo 上的应用程序。大多数应用程序都绑定了这两个接口，并没有注意到任何区别；但有些应用程序被特别编写为只期望在其中一个接口上获得特定的流量（例如，通常只在 lo 上暴露管理端点，而从不通过 eth0，或者有状态的应用程序只绑定 eth0）。这些应用程序的行为可能会受到 Istio 引导流量进入 pod 的影响。 在 1.10 版本中，Istio 正在更新 Envoy，默认在 eth0 而不是 lo 上向应用程序发送流量。对于新用户来说，这应该只是一个改进。对于现有的用户，istioctl experimental precheck 将识别出监听 localhost 的 pod，并可能受到影响，如 IST0143。 请参阅 John Howard 的文章，以更深入地了解这一变化，如何以及为什么它可能会影响你，以及如何实现无缝迁移。 Istio.io 改版 我们对 Istio.io 进行了改造，采用了全新的外观！这是 Istio 项目启动近四年以来，网站的第一个重大变化（我们将在 5 月 24 日，北京时间 5 月 25 日，庆祝这个周年纪念日！）。我们希望这些变化有助于使网站更方便用户，更容易浏览，总体上更有可读性。 Istio 官网全新改版，效果如图。 网站左下角有中英文切换功能。 感谢云原生社区 Istio SIG 翻译和维护了 Istio 官网中文文档。 这项工作由 Google Cloud 赞助，我们要特别感谢 Craig Box、Aizhamal Nurmamat kyzy 和 Srinath Padmanabhan 推动这项工作，并感谢所有帮助审查和提供早期修订反馈的人们。 请在 istio.io 资源库上提交问题，给我们任何反馈。 开放我们的设计文件 从 2021 年 5 月 20 日开始，Istio 的设计和规划文件将向互联网上的所有人开放，无需登录。此前，查看这些文件需要谷歌登录和群组成员资格。这一变化将使技术文件的分享更容易、更开放。文件将保持在与以前相同的 URL，但 Community Drive 及其文件夹将改变位置。我们将在本周内联系所有的贡献者和 Drive 成员，并告知新的细节。 弃用 在 1.10 版本中，有两个功能将被废弃。 Kubernetes 第一方 JWT 支持（values.global.jwtPolicy=first-party-jwt）将被删除；它的安全性较低，仅用于向后兼容旧版 Kubernetes。 values.global.arch 选项已经被 Kubernetes 配置中的 Affinity 设置所取代。 请参阅 1.10 变更说明以了解这些废弃的详细情况。 反馈 如果你已经将你的服务网格升级到 Istio 1.10，我们想听听你的意见！请考虑参加这个简短的（约 2 分钟）调查，以帮助我们了解我们在哪些方面做得好，以及在哪些方面还需要改进。 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/111.html":{"url":"release/111.html","title":"Istio 1.11——使用 CNI 取代 Istio Init 容器","keywords":"","body":"Isito 1.11——使用 CNI 取代 Istio Init 容器 2021 年 8 月 12 日发布，本年度的第三个版本。 CNI 插件（Beta） 默认情况下，Istio 会在部署在网格的 pod 中注入一个 init 容器。istio-init 容器使用 iptables 设置 pod 网络流量重定向到（来自）Istio sidecar 代理。这需要网格中部署 pod 的用户或服务账户有足够的权限来部署 具有 NET_ADMIN 和 NET_RAW 功能的容器。要求 Istio 用户拥有较高的 Kubernetes 权限，对于组织内的安全合规性来说是有问题的。Istio CNI 插件是 istio-init 容器的替代品，它执行相同的网络功能，但不要求 Istio 用户启用更高的 Kubernetes 权限。 CNI 插件可以与其他插件同时使用，并支持大多数托管的 Kubernetes 实现。 在这个版本中，我们通过改进文档和测试，将 CNI 插件功能提升为 Beta 版，以确保用户能够在生产中安全地启用这一功能。了解如何用 CNI 插件安装 Istio。 外部控制平面（Beta） 去年，我们为 Istio 引入了一种 新的部署模式，即集群的控制平面是在该集群之外管理的。这就解决了这样一个问题 —— 将管理控制平面的 Mesh 所有者和在 Mesh 中部署和配置服务的 Mesh 用户之间分离。运行在独立集群中的外部控制平面可以控制单个数据平面集群或多集群网格的多个集群。 在 1.11 版本中，该功能已被提升为 Beta 版。了解如何设置带有外部控制平面的网格。 网关注入 Istio 提供了网关作为与外部世界连接的方式。你可以部署 入口网关 和 出口网关，前者用于接收来自集群外的流量，后者用于从你的应用程序向集群外部署的服务输出流量。 在过去，Istio 版本会将网关部署为一个 Deployment，它的代理配置与集群中所有其他的 Sidecar 代理完全分开。这使得网关的管理和升级变得复杂，特别是当集群中部署了多个网关时。一个常见的问题是，从控制平面传到 sidecar 代理的设置和网关可能会漂移，导致意外的问题。 网关注入将对网关的管理变得与一般的 sidecar 代理相同。在代理上设置的全局配置将适用于网关，以前不可能的复杂配置（例如，将网关作为 DaemonSet 运行）现在很容易。在集群升级后，你也可以简单地通过重启 pod 将网关更新到最新版本。 除了这些变化之外，我们还发布了新的 安装网关 文档，其中包括安装、管理和升级网关的最佳做法。 对修订和标签部署的更新 在 Istio 1.6 中，我们增加了对同时运行多个控制平面的支持，这使得你可以 对新的 Istio 版本进行金丝雀式部署。在 1.10 版本中，我们引入了 修订标签（revision tag），这让你可以将一个修订版标记为 production 或 testing，并在升级时将出错的机会降到最低。 istioctl tag 命令在 1.11 中已经不再是实验性了。你现在也可以为控制平面指定一个默认的修订版。这有助于进一步简化从无修订版的控制平面到新版本的金丝雀升级。 我们还修复了一个关于升级的 悬而未决的问题—— 你可以安全地对你的控制平面进行金丝雀升级，不管它是否使用修订版安装。 为了改善 sidecar 的注入体验，引入了 istio-injection 和 sidecar.istio.io/inject 标签。我们建议你使用注入标签，因为比注入注解的性能更好。我们打算在未来的版本中弃用注入注解。 支持 Kubernetes 多集群服务（MCS）（实验性） Kubernetes 项目正在建立 一个多集群服务 API，允许服务所有者或网格管理员控制整个网格的服务及其端点的输出。 Istio 1.11 增加了对多集群服务的实验性支持。一旦启用，服务端点的可发现性将由客户端位置和服务是否被导出决定。驻留在与客户端相同的集群中的端点将总是可被发现。然而，在不同集群内的端点，只有当它们被导出到网格时，才会被客户端发现。 注意，Istio 还不支持 MCS 规范所定义的 cluster.local 和 clusterset.local 主机的行为。客户应该继续使用 cluster.local 或 svc.namespace 来称呼服务。 这是我们 支持 MCS 计划 第一阶段。请继续关注！### 预告：新的 API Istio 的一些功能只能通过 EnvoyFilter 来配置，它允许你设置代理配置。我们正在为常见的用例开发新的 API—— 比如配置遥测和 WebAssembly（Wasm）扩展部署，在 1.12 版本中你可以看到这些功能。如果你有兴趣帮助我们测试这些实现，请加入工作组会议。 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"release/112.html":{"url":"release/112.html","title":"Isito 1.12——支持 WebAssembly 插件管理","keywords":"","body":"Istio 1.12 —— 支持 WebAssembly 插件管理 这是 Istio 在 2021 年发布的最后一个版本，也是本年度发布的第四个版本，Istio 依然在按照它既定的发布节奏发展。 WebAssembly API WebAssembly 是一个重要的项目，开发了 3 年多，为 Istio 带来了先进的可扩展性，允许用户在运行时动态加载自定义构建的扩展。然而，直到现在，配置 WebAssembly 插件一直是实验性的，而且很难使用。 在 Istio 1.12 中，我们通过增加一个 API 来配置 WebAssembly 插件 ——WasmPlugin 来改善这种体验。 有了 WasmPlugin，你可以轻松地将自定义插件部署到单个代理，甚至是整个网格。 该 API 目前处于 Alpha 阶段，正在不断发展。我们非常感谢 您的反馈意见! 遥测 API 在 Istio 1.11 中，我们引入了全新的 Telemetry API，为 Istio 中配置追踪、日志和指标带来了标准化的 API。在 1.12 版本中，我们继续朝这个方向努力，扩大了对配置指标和访问日志 API 的支持。 要想开始，请查看文档。 遥测 API 概述 追踪 Metrics 访问记录 该 API 目前处于 Alpha 阶段，正在不断发展。我们非常感谢 您的反馈意见! 支持 Helm Istio 1.12 对我们的 Helm 安装支持 进行了一些改进，并为该功能在未来升级为测试版铺平了道路。 为了进一步简化使用流程，解决 最受欢迎的 GitHub 功能请求 之一，官方 Helm 资源库已经发布。请查看新的 入门 指南以了解更多信息。 这些 Chart 也可以在 ArtifactHub 上找到。 此外，还发布了一个新的精心制作的 gateway chart。该 chart 取代了旧的 istio-ingressgateway 和 istio-egressgateway chart，大大简化了网关的管理，并遵循 Helm 最佳实践。请访问网关注入页面，了解迁移到新 Helm chart 的说明。 Kubernetes Gateway API Istio 已经增加了对 Kubernetes Gateway API v1alpha2 版本的全面支持。该 API 旨在统一 Istio、Kubernetes Ingress 和其他代理使用的各种 API，以定义一个强大的、可扩展的 API 来配置流量路由。 虽然该 API 尚未针对生产工作负载，但该 API 和 Istio 的实现正在迅速发展。要尝试它，请查看 Kubernetes Gateway API 文档。 更多 默认重试策略已被添加到 Mesh Config 中，允许用户在同一位置配置默认重试策略，而不是在每个 VirtualService 中重复配置。 一个新的 failoverPriority 配置已经被添加到 定位负载均衡配置 中，允许自定义 pod 的优先级。例如，同一网络内的 pod 可以被赋予额外的优先级。 增加了新的配置，使 发起安全 TLS 更简单。 回顾：对 gRPC 原生 \"无代理\" 服务网格 的初步支持。 增加了 对 HTTP/3 网关的实验性支持。 有关完整的变更清单，请参见 变更说明。 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"big-talk/overview.html":{"url":"big-talk/overview.html","title":"栏目介绍","summary":"《Istio 大咖说》栏目简介。","keywords":"","body":"栏目简介 《Istio 大咖说》是由企业级服务网格提供商 Tetrate 冠名的以 Istio 和服务网格为主题的在全球性直播节目《Istio Weekly》的一部分。《Istio 大咖说》旨在分享 Istio 相关的开源技术及实践，开播于 Istio 开源四周年之际（2021 年 5 月 25 日），本节目定期邀请 Istio 和服务网格领域的专家参加直播与观众互动。 资源 直播间地址：Bilibili - 《Istio 大咖说》 历史视频回看：Bilibili - IstioServiceMesh 互动文档地址：腾讯文档 幻灯片归档地址：GitHub 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"big-talk/ep01.html":{"url":"big-talk/ep01.html","title":"第 1 期：Istio 开源四周年回顾与展望","summary":"《Istio 大咖说》第 1 期，2021 年 5 月 25 日，嘉宾马若飞，话题《Istio 四周年回顾与展望》。","keywords":"","body":"Istio 大咖说第 1 期 时间：2021 年 5 月 25 日晚 8 点 主持人：宋净超（Tetrate） 嘉宾：马若飞（FreeWheel） 嘉宾介绍：《Istio 实战指南》作者、极客时间《Service Mesh 实战》专栏作者、AWS Container Hero 视频回放：《Istio 大咖说》第 1 期：Istio 开源四周年回顾与展望 幻灯片归档：GitHub 互动文档：本期无 本期简介 想了解 Istio 的来历吗？想知道 Istio 自我救赎般的架构重构吗？想窥探 Istio 开发背后的趣事吗？想一起解读最新版本的新特性吗？北京时间 5 月 25 日晚上 8 点，相约 B 站，让我们一起回顾 Istio 发布四周年的点点滴滴，B 站直播间不见不散！ B 站海报-第1期 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"big-talk/ep02.html":{"url":"big-talk/ep02.html","title":"第 2 期：从微服务架构到 Istio——架构升级实践分享","summary":"《Istio 大咖说》第 2 期，2021 年 6 月 2 日，嘉宾潘天颖，话题《从微服务架构到 Istio——架构升级实践分享》。","keywords":"","body":"Istio 大咖说第 2 期 时间：2021 年 6 月 2 日晚 8 点 主持人：宋净超（Tetrate） 嘉宾：潘天颖（小电科技） 嘉宾介绍：小电科技工程师，云原生爱好者，Kubernetes contributor，Apache committer。 直播间：https://live.bilibili.com/23095515 视频回放：https://www.bilibili.com/video/BV1QQ4y1X7hP/ 幻灯片归档：GitHub 本期简介 云原生赛场已经进入了下半场的比拼，Istio 作为服务网格赛道上的明星选手，社区活跃度和用户知名度都位列前茅。但是国内环境下 Istio 的落地实践却并不多。是什么阻碍了 Istio 的落地，到底 Istio 好不好用？到底什么情况下使用 Istio 利大于弊？本次我们将站在企业用户角度上，邀请了潘天颖分享 Istio 在小电科技的完整落地经验，讲述为什么要从传统微服务架构迁移至服务网格架构，其中遇到的困难与解决方法，以及针对 Istio 的改进方案。 Istio 大咖说 第二期 扫描上图中的二维码可以跳转到《Istio 大咖说》的 B 站直播间，或者直接访问：https://live.bilibili.com/23095515 点个关注，不迷路。 你还可以扫描上图中的二维码在腾讯文档中参与直播互动，我们将会在直播最后送出由机械工业出版社赞助的《Quarkus 实战》和《Knative 实战》书籍。 听众收益 了解 Istio 现状以及使用 Istio 的利弊分析 讲述一次完整的服务迁移至 Istio 实践，可供借鉴 了解一些在使用 Istio 中经常会遇到的典型 Issue 以及解决方案 Q&A 整理 请问刚参加工作的应届生，工作涉及istio中的envoy，请问应届生应该如何提前学习Envoy？ 答：需要多了解下网络知识，还有云原生社区的Envoy中文文档可以学习，也可以参加社区的翻译活动。 在 service 特别多的时候，比如几万个 service，sidecar 是否会占用过多内存？如果是的话，可以有什么解决思路？ 答：可以通过将应用按业务组进行分组，将相互依赖的应用分开部署到不同的namespace中，然后配置sidecar配置进行配置收拢，使应用只关心特定namespace中的服务配置，个别服务通过网关调用。毕竟很难有应用会依赖几万个svc的。 我们想上Istio，请教下有什么坑？Istio1.10版本。建议上吗？架构为 SpringCloud。 答：本次分享应该回答了问题，这里很难回答。 请问针对使用dubbo框架开发的应用来说，上Istio有什么建议？老师对aeraki这个项目怎么看？Istio对于支持dubbo以及其它协议这块有什么好的支持方式？ 答：可以考虑升级下dubbo 3.0.0，istio目前已经支持dubbo协议，但是由于dubbo数据包attachment在整个数据体的最后，反序列化成本较高，需要关注下性能。还有一点是一般dubbo服务sdk比较难去掉，和 Istio功能重合太大了。不太了解aeraki这个项目。社区可以邀请项目的人来分享。 IRA开源吗？ 答：这个服务通过istio的configController机制完成，代码量不大，目前还没开源。 Mesh外的服务怎么访问Mesh内的服务？走Mesh的Ingess gateway 吗？ 答：分享中已经提到，我们通过register-helper sidecar容器，保留了迁移过程中mesh外服务通过注册中心访问网格内服务的能力。完成迁移后可以将该sidecar去掉。当然网格外的服务通过ingress也是能够访问网格内的服务的。 请问接入Istio需要开发介入什么操作变更，当前架构为Spring Cloud，注册中心为nacos/eureka，95%应用在Kubernetes上，现在需要用到流量控制，ingress gateway，熔断，限流等功能。 答：如果迁移设计的比较完美，其实可以做到开发零介入。比如我们的实践，大多数java服务只需要引入一个java jar包就可以。这个包中你可以将原来java服务底层的register sdk屏蔽，并且去除loadbalace逻辑，保留上层的接口。减少开发的接入成本，也方便Istio的推广。 上网格后springcloud consumer是否需要手动修改每个provider地址为Kubernetes service name? 答：这个建议是要替换的。但是有很多比较优雅的方式，详细见分享。 能分享下目前用wasm主要做了哪些事么？稳定性和性能方面怎么样？ 答：对流量打标处理，方便后续进行route分发。由于是使用在sidecar而不是ingress上，性能上面并没有问题。稳定性需要使用者来保证了，wasm插件切不可出错，否则会影响所有流量。 目前Istio控制面管理不友好，如果不通过自研方案的话有什么推荐么？yaml方面，目前看没有很好的gui配置。 答：社区比较主流的有kiali，如果企业支持，可以参考下一些企业支持版的Istio版本，比如TSB。 什么样的业务或者场景适合上Istio，运维工程师该注意那些点，是否有很好的方案供参考，运维如何和开发配合完善推进上Istio，我们的业务是支付打款相关的，目前是.Net。 答：Istio适不适合其实和业务关系不大，和历史技术架构关系比较大。运维工程师和开发接受Istio架构需要比较大的学习成本。如果对这个有信心，那么并没有大问题。至于.Net还是其他语言关系并不大，毕竟Istio是语言无关的。 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"big-talk/ep03.html":{"url":"big-talk/ep03.html","title":"第 3 期：如何让 Istio 变得更为高效和智能","summary":"《Istio 大咖说》第 3 期，2021 年 6 月 8 日，嘉宾杨笛航，话题《如何让 Istio 变得更为高效和智能》。","keywords":"","body":"Istio 大咖说第 3 期 时间：6 月 9 日（星期三）晚 8 点 - 9 点 直播间：https://live.bilibili.com/23095515 主持人：宋净超（Tetrate） 嘉宾：杨笛航（网易数帆） 话题：如何让 Istio 变得更为高效和智能 直播回放：https://www.bilibili.com/video/BV18o4y1y75e/ 幻灯片归档：GitHub 嘉宾简介 杨笛航，Istio 社区成员，网易数帆架构师，负责网易轻舟 Service Mesh 配置管理，并主导 Slime 组件设计与研发，参与网易严选和网易传媒的 Service Mesh 建设。具有三年 Istio 控制面功能拓展和性能优化经验。 Istio 大咖说 第三期 话题介绍 Istio 作为当前最火的 Service Mesh 框架，既有大厂背书，也有优秀的设计，及活跃的社区。但是随着 Mixer 组件的移除，我们无法通过扩展 mixer adapter 的方式实现高阶的流量管理功能，Istio 的接口扩展性成为亟待解决的问题。本次直播将分享本次分享将介绍网易自研的智能网格管理器 Slime，借助它，我们实现了配置懒加载，自适应限流，HTTP 插件管理等扩展功能，从而更为高效的使用 Istio。 听众收益 了解在实际业务中驾驭 Istio 框架的挑战 了解 Slime 的设计特点、技术路线及开源进展 了解网易解决 Service Mesh 架构成熟的经验 问答 Slime 兼容什么版本的 Istio？ 答：1.3 以后的版本都可以，网易内部使用 1.3 和 1.7 版本。 请问对于 Slime 的懒加载功能，在初始的时候是否可能存在 “冷启动”（刚开始 fence 里的服务较少，大多数依赖如果需要从兜底路由拉取需两次代理）。a) 能不能在刚开始的时候在 fence 里把所有 service 都写上（相当于配置信息全量下发），每次被调用到的服务放到 fence 的前面，等几轮全量下发配置之后将 fence 里排在后面的 service，类似于 LRU 算法）删除，这样可不可以减少两次代理的次数。b) 或者在开始前分析最近的流量关系，得到一个初始的 fence 状态。 答：a) 可以在 fence 里手动配置，也可以当做 SidecarScope 资源来用，例如已经确定了 a 到 b 的调用关系，那么可以使用如下配置作为 a 的 servicefence： apiVersion: microservice.netease.com/v1alpha1 kind: ServiceFence metadata: name: a namespace: test1 spec: enable: true host: b.test1.svc.cluster.local: stable: {} b) 可以有类似的做法，例如可以开启配置懒加载在测试环境运行一段时间，得到 fence 的初始状态然后放到线上环境去使用。 如果想给 Istio 添加更多的负载均衡策略，能否通过添加 CRD 的方式，或者通过 wasm 拓展 envoy？ 答：Slime 主要是做控制平面的扩展，负载均衡策略可能要通过 Envoy 来扩展。 有什么办法能让两个服务均在网格之中，但在他们双发调用时流量不经过 proxy 而直接访问？ 答：动态修改 iptables。 协议：有什么办法能实现满足支持其他 RPC 协议，如 thrift 和其他私有 RPC？ 答：Envoy 已经支持 Thrift 和 Dubbo 协议，但是不能比较好的路由和流量管理，可以研究下开源项目 Aeraki。 写 SmartLimiter CRD 时，ratelimite 是如何渲染成 EnvoyFilter 的，在数据面要做些什么工作？ 答：EnvoyFilter 的作用是将某一段配置插入到 xds 中的某块位置，实现 ratelimite 的功能就是要把 ratelimite 的相关配置插入到 rds 中 host 级别 /route 级别的 perfilterconfig 中。因而我们可以基于一个固定的模版去渲染 EnvoyFilter。目前自适应限流是基于 envoy 官方的限流插件的，不需要数据面做额外的工作。 第一次访问为什么会走到 global sidecar？ 答：第一次访问还没有服务依赖拓扑，调用者没用被调用者的服务发现和路由信息，需要 global sidecar 作为兜底代理 EnvoyFilter 有执行顺序吗，比如设置了 RequestAuthentication 和 AuthorizationPolicy，在 filter 里面有执行顺序吗？ 答：LDS 中的插件次序就是执行顺序。 Sidecar 和 应用容器的启动顺序，一定是 sidecar 先启动吗？ 答：在安装时开启 holdApplicationUntilProxyStarts 可以确保 sidecar 容器先启动（Istio 1.7 +）。 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"big-talk/ep04.html":{"url":"big-talk/ep04.html","title":"第 4 期：如何让 Istio 在大规模生产环境落地","summary":"《Istio 大咖说》第 4 期，2021 年 6 月 23 日，嘉宾陈鹏，话题《如何让 Istio 在大规模生产环境落地》。","keywords":"","body":"Istio 大咖说第 4 期 分享时间：2021 年 6 月 23 日（周三）晚 8 点到 9 点 议题名称：如何让 Istio 在大规模生产环境落地 主持人：宋净超（Tetrate） 分享嘉宾：陈鹏（百度） 直播间地址：https://live.bilibili.com/23095515 回放地址：https://www.bilibili.com/video/BV18M4y1u76t/ 提问地址：https://docs.qq.com/doc/DRUZSbHVkck9Wc0V4 PPT 下载：见 Istio 大咖说往期节目列表 陈鹏，百度研发工程师，现就职于百度基础架构部云原生团队，主导和参与了服务网格在百度内部多个核心业务的大规模落地，对云原生、Service Mesh、Isito 等方向有深入的研究和实践经验。 Istio 大咖说第四期-陈鹏 分享大纲 百度服务治理现状 & Istio 落地挑战 深入解读如何让 Isito 在大规模生产环境落地 实践经验总结 & 思考 通过本次分享你将了解当前 Isito 落地的困境和解决思路。 Q&A 请问，Istio 组件，包括数据面和控制面发生故障时，有哪些快速 fallback 的方案，之前阅读了大佬的相关文章，这块好像没有细讲，谢谢！ 相对来说， 数据面 fallback 更重要一些，因为它直接接流量。最好是能结合流量劫持环节，让 sidecar 故障时，系统能自动感知到，流量自动 fallback 到直连模式，具体思路可以参考分享内容，其次 sidecar 的监控也是必要的，用来辅助感知和处理故障。控制面故障短时间内不会对流量转发造成影响，也可以通过部署多副本来应对故障。 有些服务 QoS 想设置 guaranteed，但是 istio 的 sidecar 不是 request 和 litmit 一样的。要 guaranteed 的话必须 sidecar 的 QoS 相关配置也要改。你们是怎么做的？如果改，建议设置为多少？感谢大佬！ 我们通常 sidecar 内存 limit 200M，cpu limit 是业务的 10% 到 20%，但我觉得这个没有通用的参考数据，需要结合业务场景压测拿到数据，更为科学。 你好。作为业务方的技术负责人，被中间件团队推动上 Istio，目前带来的收益主要是服务治理方向的。从业务迭代效率的角度看，上 Istio 的增量收益抵不上迁移成本的代价；另外还降低了业务方工作的技术含量。请问有没有实践中，业务方在 Mesh 化的过程中获得显著收益的例子？ 不能只关注 mesh 自身的能力，还要深入了解业务，最好是能挖掘业务最痛点的问题，比如原有框架治理能力比较弱，或者可视化能力弱，或者变更效率低等，业务最痛点的问题对于 mesh 来说可能是普通的能力，但却会给业务线带来很大的提升。 为什么没有采用 mcp over xds 的方案？ 而是独立搭建了 API server 和 etcd？ api server + etcd 可以比较容易的独立部署，能复用尽量复用。 pilot 的多业务方案是怎么做的？ 如果是多个 pilot 的话，那独立的 Kubernetes API server 和 etcd 也要多个么？ 我们目前的实践是 api server + etcd 是一套，pilot 根据业务线多套，当然 api server + etcd 也可以是多套，可以根据系统性能以及数据隔离等需求灵活部署。 Envoy 的 brpc 改造方案，会考虑开源么？ 怎么 follow 后续社区 Envoy 的最新代码呢 brpc 本身也是开源的，Envoy 只是做了集成。社区版本更新很快，具体多久更新一次其实很难抉择，需要考虑架构升级成本，以及社区版本的成熟度，这块我也没有太好的建议。 内部的 mesh 方案，考虑通过百度云的方式对外输出么？ 一些高级策略能力会逐步输出到公有云产品。 随着 istio 新版本的不断发布，内部使用的版本是否跟进了开源新版本，跟进社区版本升级有什么经验分享？ 同上，社区版本更新很快，具体多久更新一次其实很难抉择，需要考虑架构升级成本，以及社区版本的成熟度，这块需要结合团队现实情况考虑升级时机。 Envoy filter 管理麻烦的话，nshead、dubbo 等多协议支持是怎么实现的？在 pilot 中是如何管理的？ 我们内部是直接在 pilot 内部实现支持，类似于 http 的功能。 引入两个 sidecar 后问题定位的成本和难度会大福增加，这块有什么经验可以分享 一方面 Envoy 自身提供了丰富的接口，可以暴露内部很多的状态，另一方面也需要和自有监控基础设施对接。 Sidecar 带来的额外成本问题谁来买单？业务认可吗 这个其实需要和业务团队明确，额外的资源成本是需要业务买单的，但对于内部业务，具体的成本可以比较低，业务普遍是能接受的。 Sidecar 可以使用的的资源配额是怎么分配管理的，动态的还是静态的，有什么经验 不同的业务场景可能不太一样，内存大概在几百 M，CPU 一般是是业务的 10% 到 20%，但最好是要根据业务场景进行压测，得到数据。 Sidecar 的监控是怎么做的？ 权限，成本方面可能都有一些疑问 Envoy 本身会暴露自身指标，对接相关的监控即可。 Naming 这个 agent 和框架非常不错，请问 Naming 可以支持负载均衡么， 也就是 PodX 访问 PodY 的时候，naming 不要返回 PodY 真实 IP，而是返回负载均衡的 VIP 给 PodX; 十分感谢 - Ken 目前没有这么做，直接返回了 Envoy 的 loopback ip 来做流量劫持。 这种架构的话，PodX 主动出公网的逻辑是怎样的呢，也是通过 ip-masq 做 NAT 吗？ 目前主要做内网服务的 mesh，这块没有太多经验，十分抱歉～ Naming agent 是部署在哪个容器？ 是一个主机部署一个的单机 agent，工作在主机网络上的。 Pliot 在落地过程中部署模型，大规模 Envoy 注册后，是否存在一些性能瓶颈，有什么优化的经验？ 可以增加 pilot 的副本数，来应对大规模 Envoy 的链接，另外控制面处理逻辑也有很多可以优化的地方，来优化从 API server 拿到数据之后的计算过程，这部分需要对 pilot 代码有一定开发经验和熟悉程度。 虽然老师不一定有关注这一块，但也提个问题看看吧。 Envoy 流量劫持是在 userSpace 还要经 TCP 协议栈其实损耗非常大的，后续 Envoy 有考虑 byPass Kernel，直接传包给网卡驱动提速么（例如 DPDK、SPDK） 这个一方面要考虑成本问题，比如内核和硬件是否满足，另一方面也要评估收益，比如流量劫持这一部分虽然优化了，但是缩减的耗时对于整个请求链路的占比是否足够明显。 流量都经过 sidecar 后，sidecar 在 trace 这方面是怎么考虑和设计的？ Envoy 本身支持 trace 相关的功能，这块其实是需要业务 sdk 中来进行支持，必须要透传 trace 相关的信息。 对于 inbound 流量的限流是如何设计的呢？ 可以使用 EnvoyFilter CRD，给被调用方 inbound listener 插入 LocalRateLimiter 对应的 filter 来实现。 私有协议如果要变更的时候，是不是要级联更新？ 抱歉，这个问题没太看明白～ 支持服务治理的配置灰度下发吗？可以简单说下实现方案吗 内部其实是实现了，方案比较复杂，简单来说就是控制面自己控制 xds 下发，会先挑选部分实例生效，然后再给全部实例下发。 你们 内部对于 istio deployment 里的 version 字段在落地时有大规模使用吗？我们最近在基于 istio 做灰度发布，但是每次灰度都要给他一个版本号，导致完成之后 deployment 名称就从 v1 变成 v2 以此累加，这样还会导致 deployment 本身的回滚功能失效。 我理解是不是只需要 v1 和 v2 就够了，先灰度给 v1，没问题的话 v2 也生效，这时候 v1 和 v2 策略就打平了。下次恢复依然还是这个流程，好像不需要一直叠加版本好吧，不确定我理解的对不对～ Envoy 对于我司来说技术储备其实不是很够， 请问贵司刚上线的时候， Envoy 有没有遇到哪些问题。 特别是稳定性和故障方面。 如果能建议一下 Envoy 应该如何监控，那就 perfect. Envoy 本身比较复杂，上线初期一定会遇到问题，最好是能结合流量劫持方案，做到 Envoy 故障自动 fallback，思路可以参考分享内容。监控的话，Envoy 自身会暴露 stats 接口，比较容易接入监控系统。 对于 dubbo 的泛化调用，探针会实时检测调用关系的变化么？如果 sidecar 还没有被生成，这个时候流量请求阻塞怎么处理呢？一直等待还是直接拒绝？如果服务是新的请求呢？ 泛化调用这种比较灵活的方式，我们目前也没有很好的支持，一个思路是可以提前手动配置好调用关系。 link 模型解决 xDS 问题，可以再详细介绍一下整个逻辑链路么？例如 consumer 和 provider 的 link 数据是怎么获得的 目前内部大规模落地的方案中，是需要用户在产品上显示定义的。 2ms 的 Envoy 额外消耗，请问是怎么查看的呢？curl endpoint 跟 curl Envoy 做一次对比么 官方的测试方法没有详细研究过，自己测试的话，可以用经过 Envoy 和 没经过 Envoy 的耗时 diff，也可以在程序里打点来看。 Envoy 注入后业务 pod 会存在 2 个 container， 那么 Envoy 的配额是怎么限制的呢？ 比如限制 4 核心可能就是 2 个容器（1 个 pod）里面的配额了； 可以参见上面的回答 就是我们在落地的时候，会遇到部分服务有 sidecar，部分没有 (服务 A 会被其他 10 个服务调用)，一般如何去判断配置设置在哪里，是在 outbound（其他 10 个服务部署 sidecar）处还是 inbound 处（服务 A 部署 sidecar）。这个有没有什么比较好的实践？ 这个需要结合流量劫持方案，做到有 sidecar 就过 sidecar，没有就走直连，具体思路可以参考分享内容。 Envoy 如何实现长连接的动态开关的？ 这个问题比较好，我们是通过让 Envoy 重新生成一个 listener，更改了 listen 的地址，让调用 Enovy 的 SDK 感知到，并重新链接。 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"big-talk/ep05.html":{"url":"big-talk/ep05.html","title":"第 5 期：腾讯云服务网格生产落地最佳实践","summary":"《Istio 大咖说》第 5 期，2021 年 6 月 30 日，嘉宾钟华，话题《腾讯云服务网格生产落地最佳实践》。","keywords":"","body":"Istio 大咖说第 5 期 分享时间：2021 年 6 月 30 日（周三）晚 8 点到 9 点 议题名称：腾讯云服务网格生产落地最佳实践 主持人：宋净超（Tetrate） 分享嘉宾：钟华（腾讯云） 直播间地址：https://live.bilibili.com/23095515 回放地址：https://www.bilibili.com/video/BV1th411h7Zr/ 提问地址：https://docs.qq.com/doc/DRUZSbHVkck9Wc0V4 PPT 下载：见 Istio 大咖说往期节目列表 Istio 大咖说第五期 钟华，腾讯云高级工程师，Istio contributor，Dapr contributor, Tencent Cloud Mesh 技术负责人。专注于容器和服务网格，在容器化、服务网格生产落地和性能调优方面具有丰富经验。 通过本次分享了解大规模场景下，Istio 性能调优和最佳实践，包括 xDS 懒加载，控制面负载平衡，控制面灰度升级，Ingress gateway 优化等。 分享大纲 Istio 生产落地挑战 腾讯云服务网格全托管架构介绍 大规模服务网格性能优化 Istio 生产落地最佳实践 Q&A Istio 的 gateway 对比 ambassador 的差异在哪？是否在 Istio 的基础上增加 api 网关这一层？增加后能填补哪些缺陷？ 答：Ambassador 本身也是基于 envoy 之上的一个云原生网关产品， 本身包括控制平面；api gateway 范畴包括一些 Istio ingress gateway 不具备的功能，比如 api 生命周期管理，api 计费，限速，监控，认证等。所以 api gateway 本身有存在的必要，不过 API Gateway 需求中很大一部分需要根据不同的应用系统进行定制。 我们有客户将 kong， openresty 等 api gateway 和 Istio ingress gateway 结合起来用。 对于多集群 Istio 部署了解到架构图如下所示，由单个控制平面管控所有集群，这里如果有 k8s 集群间网络不通，pod 与 pilot 交互链路是？ 答：数据面 k8s 间不互通，不会影响 控制面和数据面的通信。 Istio 多集群的前提是：多 k8s 之间要互通，可以是 pod 扁平互通，或者通过 Istio gateway 互通。 Istio Envoy Sidecar 使用 iptables 劫持流量，一定规模环境下性能损耗较大，排障复杂，很多大厂都是自研 Envoy，比如蚂蚁金服、新浪、腾讯、华为等，自研的 Envoy 是使用什么来劫持流量呢，亦或者说自研的 envoy 解决了原生的 envoy 哪些缺陷呢？ 答：常见的有三种： 1）uds：数据包不过协议栈，性能高，但只适合私有 mesh，因为需要应用面向 uds 编程。不适合公有云。比如美团，字节在使用这种方案。 2）localhost+port：使用 port 代表不同的服务，通常需要拦截服务发现流量，再重新规划服务到端口映射，有一定管理成本，比如百度在使用这种方案。 3）ebpf：在内核 socket ops 挂载 ebpf 程序，应用流量和 envoy 流量在 这个互通，流量不经过协议栈，性能高，对用户透明，但技术门槛高，对内核有版本要求。目前腾讯云 TCM 在小范围推广。 长连接的情景下懒加载又是如何实现的？Workload 流量如何重定向到 egress，通过 passthrough？ 1）目前 lazy xds 对长连接没有特殊处理，用户需要权衡一下，首跳长连接性能 vs 数据面内存开销，以此决定是否使用长连接，大家如果对长连接 lazy xds 有想法，欢迎联系我。 2）没有走 passthrough，请看 lazyxds 架构图上第二步，是会给 workload 2 下发具体的重定向规则，也就是指明哪些服务流量要到 lazy egress。 钟老师您好， Istio 属于较新的技术，能否推荐一下监控应该怎么做， 或具体监控哪些指标？ 另外，下图步骤 10， Istiod 更新的时候是全量所有 workload 都更新吗， 还是只更新 wordload1 ? 1）mesh 监控包括三个方面：metric, tracing，logging, TCM 技术选型偏云原生：metric 使用 prometheus, tracing 使用 jaeger collector， logging 是自研的技术。另外也用到了腾讯云上的监控服务。 2）只更新 workload1，注意架构图上的第八，sidecar 里会指定具体的 workload。 Istio 目前的性能优化有什么实践经验吗 答：TCM 团队之前在 kubeconf 上有数据面性能分享，请参考：深入了解服务网格数据平面性能和调优。 请问 isitod 的稳定性有什么实践经验可以分享吗，failback，failover 容错机制是怎么实现的？ 本次分享包括 2 个 Istiod 稳定性实践：如何保证 Istiod 负载平衡，如何对 Istiod 做灰度升级 数据面 failback，failover 本身是 envoy 的能力，Istio 会给 eds 设置 priority， 这个值表示和当前 pod 的亲和度（地域和区域），（如果开启就近访问）服务访问会优先访问 priority 为 0 的 endpoint， 如果为 0 的 endpoint 都失效了，访问会 failover 到 priority 为 1 的 endpoint，接下来是 priority 为 2 的，逐级失效转移。 Istio 与已有的基础设施 (注册中心等) 如何整合，是使用 mcp 还是 k8s api server 实现 答：之前我们尝试过 mcp，不过比较难调试，目前我们更推荐使用扩展 service entry 方式，参考我们开源的 dubbo2istio 或 consul2istio。 TKE 注入 sidecar pod 会从 1 个容器升级为 2 个容器，请问 pod 对集群内其他 pod 访问的链路是怎么走的呢？ 20:28 说到控制面板资源 HPA 后依然会紧张，能否建议下 ISTIOD 的资源应该如何设计么， 比如 n 个 pod 对应 1 个 Istiod。 client 业务容器 ->client pod iptables->client envoy （将 service ip 转成 pod ip） -> node (iptables 不做 service nat 了) -> server pod iptables-> server envoy -> server 业务容器 需要结合业务做压测，通常建议可以把 request 设小一点，把 limit 设大一点。 isito 的部署模型是怎么样的？是每个业务部署一个 isitod 集群，还是多个业务共享？ 答：TCM 托管场景下，每个 mesh 有一个 Istiod。Istiod 按照 namespace 隔离。 namespace 是如何划分的，是按照业务来划分吗？ 答：TCM 场景下，是的，通过 namespace 隔离多租户的控制面。 Istio 使用 envoyFilter 做限流，可以在 inbound 上根据 url 前缀匹配或者接口级别的维度做限流么？目前看只能在 outbound 上引用 virtualService 里面的配置，inbound 只能限制总流量。 答：目前社区应该不支持 url 级别的限流，需要自研。这个需求是刚需，我们可以一起调研下解决方案。 CRD 托管的原理能详细介绍下吗？ 答：核心使用的是 kubernetes aggregation 技术，把 Istio CRD 作为 kubernetes 的外部扩展。 当用户读写 Istio crd 时， api server 会将流量路由到我们指定的外部服务，我们这外部服务实现了 crd 的托管。 Envoy 如何做热更新？怎么在容器内注入新版本的 Envoy？ 答：热更新核心是通过 UDS（UNIX Domain Socket），可以参考下 openkruise 解决方案，不过该方案只能解决仅有镜像版本变化的更新，对于 yaml 变化太大的更新，目前不好处理。 业务容器已经启动接收流量了，而 envoy 还没完成 eds 的下发，出现流量损失？Istio 是否会出现这种情况？ 答：会的，所以需要遵循 make before break，核心原因在于：目前 Istio 实现中，没法知道 规则下发是否完全生效。 目前的姑息办法是 make before break + 等待一定时间。 如何支持 subdomain-*.domain.com 这样的 host 规则？Envoy 是不支持的，有没有方法可以扩展 答：目前的确不支持，建议去社区提 issue，参与共建。不过 Istio 的 header match 支持正则，可以尝试使用 host header，或者 authority 属性，需要验证一下。 Istio 可否实现类似于 dubbo 服务的 warmup 机制，动态调整新注册 pod 的流量权重由低到正常值？ZPerling 答：Envoy 社区有提案，目前没有完成：issue #11050 和 issue 13176。 Mysql 和 mq 可以做版本流量控制吗？他们的流量识别怎么做呢？ 答：目前不行，这是 Istio 的软肋，envoy mysql filter 功能比较基础，关注下 Dapr 这个项目。 原来的 SpringCloud 项目 服务注册发现 & 配置中心用的 consul，如果切换 Istio 的话，服务发现和配置中心要怎么支持？ 答：注册发现考虑下 consul2istio，另外 SpringCloud 组件可能需要做一些减法，去掉一些 Istio 支持的流控能力组件。 SpringCloud 项目 通过 K8s 集群部署，切换到 Istio，原来业务依赖的中间件通信方式需要改变？原本的流量如果直接切换到 Istio 风险较高，有没有一键下掉 Istio 的开关或者这种机制：降低有问题流量降级切换到原来的部署架构？ 1）可能改变的通信方式：主要是服务发现过程改变。Istio 支持透明接入，通常中间件的通信方式不会受影响。 2）这个能力的确会给刚开始 mesh 化的业务带来信心，开源 Istio 没有这个能力，参考之前百度陈鹏的分享。 使用 traefik 作为边缘代理，Istio 来管理服务内部的流量。traefik 转发策略是直连 pod，而不是走 k8s 的 service，如何使用 Istio 来管理到达服务的流量？ 答：抱歉我对 traefik 并不熟悉，不过大概看了这篇在 Istio 服务网格中使用 Traefik Ingress Controller，流量从 traefik 出来是经过了 envoy，在这里应该还可以做服务治理，后面我再研究下。 目前 Istio 版本缺失限流功能，这部分要怎么支持？ 答：目前 Istio 支持 local 和 global 两种方式，不过 local 无法多 pod 共享限频次数，global 性能可能不一定满足用户需求。 目前社区应该不支持 url 级别的限流，需要自研。这个需求是刚需，我们可以一起调研下解决方案。 现有 K8S 集群业务切换到腾讯云 Istio 部署需要做哪些操作？成本高？ 答：看当前业务的技术特征，如果是 http、grpc+ k8s 服务发现，迁移成本比较低，如果有私有协议，会有一定难度。 是一套 k8s 对应一套 Istio，还是一套 Istio 对应多个 k8s 集群？多集群是怎么做的？ 主要看业务需求，如果有跨集群业务互访，或者跨集群容灾，就可以考虑使用 Istio 多集群方案。 多集群实现可以参考我之前的分享：Istio 庖丁解牛 (五) 多集群网格实现分析 和 istio 庖丁解牛 (六) 多集群网格应用场景。 Istio 下的服务限流方案？ 答：目前支持 local 和 global 两种方式，参考 Enabling Rate Limits using Envoy，另外网易 slime 中有动态的限流方案。 Istio 到现在都不支持 path rewrite 的正则，这块是否有一些社区的方案支持，因为这个策略在实际的业务中还是很常见的 答：目前的确不支持，建议去社区提 issue，参与共建。 多网络单控制平面的情况下，从集群如果没有某服务的 Pod 的话，该集群其他 Pod 通过域名访问主集群 pod 的话从集群必须有空的 svc 吗，有其他什么方案实现吗 智能 dns 方案成熟了吗？Pilot Agent 不是有 DNS Proxy 么 答：Istio 1.8 提供的 智能 DNS 可以解决这个问题，1.8 里有 bug， 1.9 修复了，目前我们有生产客户在用了，目前看起来生产可用，可以尝试。 切换到 istio, 原来业务依赖的中间件通信方式需要改变？MySQL Redis Consul Kafka 答：Istio 对 db mesh 支持功能不多，通信方式不需要改变。 推荐使用哪个版本的 Istio？ 答：建议使用次新版本，比如现在 1.10 发布了，建议使用 1.9；未来 1.11 发布了，就要着手升级到 1.10。 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"big-talk/ep06.html":{"url":"big-talk/ep06.html","title":"第 6 期：Envoy Proxy 在线答疑","summary":"《Istio 大咖说》第 6 期，2021 年 8 月 3 日，嘉宾周礼赞，话题《Envoy Proxy 在线答疑》。","keywords":"","body":"Istio 大咖说第 6 期 分享时间：2021 年 8 月 3 日（周二）中午 12:30 到 14:00 议题名称：Envoy Proxy 在线答疑 主持人：宋净超（Tetrate） 分享嘉宾：周礼赞（Tetrate） 直播间地址：https://live.bilibili.com/23095515 回放地址：https://www.bilibili.com/video/BV1d64y1x7yn 提问地址：https://docs.qq.com/doc/DRUZSbHVkck9Wc0V4 我们在过去两个月内已经陆续举办了 5 期《Istio 大咖说》，直播过程中很多观众反馈想要了解下 Envoy，有很多关于 Envoy 的问题却没有人可以来解答，而 Envoy 作为 Istio 中默认的数据平面，可以说如果你搞懂了 Envoy 就算把 Istio 搞懂 80%了。这次我们邀请了来自企业级服务网格提供商 Tetrate 公司的周礼赞，他是 Envoy 的核心 maintainer，之前也在云原生社区中分享过一次《云原生学院第 17 期：Envoy 调试流量的常用技巧》。 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"big-talk/ep07.html":{"url":"big-talk/ep07.html","title":"第 7 期：基于 Envoy/Istio 的云原生 API 网关 —— 开源项目 Hango 的设计与实现","summary":"《Istio 大咖说》第 7 期，2021 年 8 月 25 日，嘉宾韩佳浩，话题《基于 Envoy/Istio 的云原生 API 网关 —— 开源项目 Hango 的设计与实现》。","keywords":"","body":"Istio 大咖说第 7 期 分享时间：2021 年 8 月 25 日（周三）晚上 8:00 到 9:00 议题名称：基于 Envoy/Istio 的云原生 API 网关 —— 开源项目 Hango 的设计与实现 主持人：宋净超（Tetrate） 分享嘉宾：韩佳浩（网易轻舟） 直播间地址：https://live.bilibili.com/23095515 回放地址：https://www.bilibili.com/video/BV1YL411b7e6/ 提问地址：https://docs.qq.com/doc/DRUZSbHVkck9Wc0V4 PPT 下载：见 Istio 大咖说往期节目列表 讲师简介 韩佳浩，网易数帆资深研发工程师，主导 Hango 网关开源研发及设计，负责网易数帆轻舟 API 网关集团内部大规模落地及产品化建设。具有三年网关相关研发及大规模实践经验。 话题介绍 云原生架构演进下，更多的业务着重于 API 的统一暴露，API 网关便成为 API 统一接入的必备组件。本次分享主要从云原生概念出发，探讨云原生模式下 API 网关的选型之道；介绍网易研发的高性能、可扩展，功能丰富的云原生 API 网关 Hango 的设计之道以及落地实践。 问答 Envoy体系学习图谱，现在是整体文档都看完有用到时再翻文档 答：可以关注社区动态，学习思路路线上可以根据自己想对 Envoy了解的程度按照以下线路进行：了解 Envoy 基本架构 -> 使用 Envoy 常用特性 -> 尝试扩展 envoy -> 对 Envoy 做深度定制，另外 Tetrate 即将推出免费的 Envoy 教程，敬请关注。 Hango项目与网易轻舟项目是什么关系？开源版么？ 答：网易轻舟项目包含轻舟微服务、轻舟API网关、轻舟容器等产品，轻舟API网关是Hango项目的商业版。 Ingress Controller与API Management是否有必要合为一个产品？ 就是 k8s 资源，意思两个产品位置是否需要合一？ 答：具体需要看网关的定位，如果作为微服务网关的话，不建议合为一个产品；如果承担ingress功能，可以合一。 使用 Envoy 以网关的形式和以 Sidecar 的形式做服务治理有什么区别，使用场景分别是什么呢？以网关的形式做东西南北向流量的服务治理的方案可行吗？ 答：网关主要做南北流量治理；Sidecar承担集群东西流量治理。在大规模场景下，不建议网关作为东西流量治理；服务调用关系简单，API规模有限可以。 Hango必须配合Istio一起使用吗？ 答：推荐使用Istio, 仅单独使用网关数据面丧失网关动态配置能力，自身静态配置复杂度也大大提高。 接问题5，如果是可以独立使用，在k8s内额外创建一个网关，这个网关目的是什么，这在集群内服务之间互访的时候等于破坏了Kubernetes本身的svc特性，consumer服务找这个网关所注册的服务？能否举例一个具体的场景。 答：不推荐独立使用，网关的功能对外统一暴露集群内API。网关暴露的意义，一部分是代理，另一部分是丰富的治理功能以及多维度的指标监控。 加载 Lua后性能有下降吗？ 答：简单的插件，性能基本在20%损失。 边缘网关有哪些场景？看到ppt里有写，但是没有讲。 答：类似集群中的统一API暴露，只是不需要额外的用户配置。 性能没太看懂，9wqps是几台机器？几c？ 答：容器：8c8g 物理机：56c256g lua 怎么保证脚本安全？隔离性怎么样？写个while true 会不会把整个网关搞崩？ 答：lua的插件链的异常不会导致主线程crash，异常后跳过逻辑，执行之后的插件链。 大规模场景下，踩过哪些坑 答：升级的平滑度以及线上规模的预估。 可以认为是在Istio gateway + virtualservice的一个升级版么？ 是不是用了这个网关我就可以不用Istio gateway了？ 答：是的 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "},"big-talk/ep08.html":{"url":"big-talk/ep08.html","title":"第 8 期：小红书服务网格大规模落地经验分享","keywords":"","body":"Istio 大咖说第 8 期 分享时间：2021 年 11 月 9 日（周二）晚上 8:00 到 9:30 议题名称：小红书服务网格大规模落地经验分享 主持人：宋净超（Tetrate） 分享嘉宾：贾建云（小红书） 直播间地址：https://live.bilibili.com/23095515 回放地址：https://www.bilibili.com/video/BV12b4y187ae/ 提问地址：https://docs.qq.com/doc/DRUZSbHVkck9Wc0V4 PPT 下载：见 Istio 大咖说往期节目列表 讲师简介 贾建云，小红书 Kubernetes 云原生工程师，负责小红书服务网格相关工作。主导设计了小红书服务网格落地方案，对于大规模服务网格落地、调优有丰富的经验。 话题介绍 小红书基于 Istio 的服务网格方案和架构设计 小红书对于 Pilot、Envoy 做的特性增强 小红书落地服务网格碰到的性能/Bug 问题 听众收获 了解小红书服务网格关于流量拦截、thrift 协议、懒加载等做的特性增强，同时了解在大规模落地服务网格过程中碰到的控制面性能问题，以及 ServiceEntry 场景下 pilot 存在的 Bug。 问答 我们在落地Istio 中碰到一个坑是 envoy 的 connection idleTime 和各种语言的 keepalive 时间不同，在大量使用长连接（http1.1）的情况下，可能会出现客户端用现有的长连接发起请求，但是服务端连接刚好超时回收了，导致会有部分请求 503（报错是 connection reset），在 Istio 社区也看到了这类的 issue，但是都没发现一个合适的解决方案。Istio 默认内置的重试条件中不包括 connection reset 这种情况，可能是害怕对非幂等请求的重试。不知道小红书内部有没有类似的问题？ 答：这个问题可以参考Envoy官网关于超时时间设置的最佳实践。 Envoy是否考虑降级，以应对envoy异常时跳过sidecar直接访问服务，不知道是否有类似经验？ 答：我们目前是通过监听实际端口来做流量拦截的，这样当出现问题之后我们会让sdk把流量切换到中央sidecar。这种流量回滚方式与我们的流量拦截方式强相关，同时也对sdk有一定入侵，可以看一下小红书关于流量拦截方案的介绍。 xds 和 eds 分开会不会有数据不一致的问题？ 答：不会有问题 有没有使用webassembly开发扩展？ 答：小红书暂时没有使用wasm，扩展是直接开发envoy filter。 配置灰度下发解决思路是什么？ 答：跟我们sidecar灰度升级的思路比较一致，通过创建cluster/ns/service粒度的升级任务，由pilot决定配置要下发给哪些sidecar Envoy引入brpc是替换了Envoy哪些部分？ 答：不算事替换吧，是想做到自由切换线程模型，引入bthread。 虚机服务（通过域名+nginx+tomcat）如何解决服务网格的灰度上线？ 答：虚拟机跟pod应该是一样的，通过创建dr维护版本信息，然后配置流量配比。 手动维护服务依赖的话还算懒加载吗？ 答：严格意义上面不算了。但是本质上都是为了做服务可见性。 懒加载中hosts依赖的serviceEntry信息是不是依然要全局envoy下发？ 答：特定服务的所有实例/流控配置是全量的，这个跟pilot实现有关，目前社区的新版本已经在开发增量推送了，可以关注一下。 不拦截入流量的话要做 inbound 的策略怎么办？ 答：原生的方案inbound本身也没有什么流量治理的特性，就是流量转发，所以我们不担心不拦截inbound会有流量治理能力的缺失。主要是担心可观察性会有影响，目前期望通过SDK补齐丢失的指标。 对 Istio multi-tenancy有支持增强吗？ 答：小红书内部对多租户没有什么诉求，这个应该是公有云比较关心。 Thrift Proxy 的路由变化后会导致重建 Listener，线上业务可以接受客户端存量链接在路由规则变化后被断开吗？ 答：目前业务方可以接受，我们是告知过这个事情的。另外就是社区已经有envoy thrift filter支持rds的pr，合并到主干之后我们会升级，届时就没有问题了。 调用的下游过多的情况下，端口的冲突怎么解决？ 答：端口不会冲突，一个Pod内部依赖的服务不会重复，每个服务都有唯一的端口。但是主机网络会存在端口冲突的情况，目前我们的方案就是让用户改为非主机网络。 懒加载中serviceEntry+sidecar中如何支持按照route等方式配置http路由信息，就像virtualserver中支持的httproute功能？ 答：使用serviceEntry+sidecar不影响vs等的使用。两个东西没有太大关系。 老师提到了小红书用到了开源项目 Aeraki 来管理 Thrift 协议，请问这部分后续的开源计划？ 答：后续会有团队小伙伴小红书分享关于aeraki做的扩展，但是应该不会合并到aeraki，内容偏小红书定制。 流量拦截中还是用到了iptables(tproxy)模式，性能上会不会依然受影响？ 答：会有影响的，但是用了tproxy模式会好一些。 有没有Envoy数据面性能的参考数据，总体上和业务容器的平均占比会是怎样的，cpu 和内存呢？ 答：按照我们内部一个业务的压测，单跳Envoy延迟增加2ms。Envoy大概占用0.5核，300m左右内存。后续我们会压测高QPS业务，届时我再补充数据。整体来看配置了懒加载envoy资源吃的不多。 灰度下发的方案，不同sidecar配置的diff是保存在那个地方？ 答：存储在mysql。 Istio 通过 virtualservice 做灰度的话，基于流量比例的灰度无法做到 session sticky，这个有最佳实践吗？ 答：这个没有。目前小红书的灰度是通过注册中心来实现的。 性能测试数据如何？ 答：参考问题17。 为什么不用 service 而用 serviceentry 呀？小红书内部没有使用 k8s service 吗？ 答：小红书内部不用service，而且serviceentry可以支持虚机。 老师能否介绍下小红书的Service Mesh发展到现在的程度，大概是多少人的团队，做了多久？ 答：目前4个人，大概做了半年。 Envoy延迟的长尾情况呢？ 答：还是比较明显的，这个跟Envoy线程模型有关吧。但是引入backuprequest会好很多，来自百度的内部实践。 大佬微信发下？ 答：请加入云原生社区 Istio SIG 交流，大佬在群里。 原生 Istio 自动注入会跳过主机模式host的pod？ 答：出于安全考虑 Istio一般也不敢直接在虚机上面拦，比较危险，最好还是不要用主机网络吧。非要用的话只能修改webhook吧。 大佬服务注册这边是什么方案注册的 答：公司内部自研的注册中心，细节不太清楚，后续可能有同事分享小红书注册中心。 请问sidecar热升级前后，通过istioctl ps 查看proxy的版本有变化吗？ 答：不会有变化。版本号是我们自己在Envoy开发的api，跟istioctl ps哪个版本没关系。 本站基于 CC 4.0 协议 发布 | 云原生社区 all right reserved，powered by Gitbook Updated at 2021-11-24 07:15:09 "}}